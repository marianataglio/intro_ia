{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "07289bb0",
      "metadata": {},
      "source": [
        "# Trabajo integrador - Parte 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42989be4",
      "metadata": {},
      "source": [
        "# Aprendizaje Supervisado\n",
        "\n",
        "**Nombre**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8387800e",
      "metadata": {},
      "source": [
        "## Problema de regresión"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "dc5e74de",
      "metadata": {},
      "source": [
        "Para la creación de los datasets y la manipulación de los mismos vamos a trabajar directamente con dos módulos includios en la carpeta utils.\n",
        "\n",
        "En esta podemos encontrar:\n",
        " - generate_data: Esta función wrappea el método de _make_regression_ de scikit learn para devolver un dataframe con un problema de regresión basado en sus parámetros.\n",
        " - generate_outliers: Esta función genera outliers livianos y pesados en función de los parámetros que le demos de entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a033541",
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.data_generation import generate_dataset\n",
        "from utils.data_manipulation import generate_outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abc333db",
      "metadata": {},
      "source": [
        "### Ejemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a902d24a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fee49e6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Vamos a crear un dataset primero.\n",
        "\n",
        "data = generate_dataset(\n",
        "    n_samples=1000,\n",
        "    n_features=5,\n",
        "    n_informative=2,\n",
        "    n_targets=1,\n",
        "    noise=0,\n",
        "    output='dataframe'\n",
        ")\n",
        "\n",
        "## esto nos genera un dataset que contiene 5 features, 2 de los cuales son informativos, y 1 target.\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c0df0dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "## vamos a visualizar estas variables\n",
        "## creamos una figura de matplotlib que contenga 5 subplots, uno por cada feature:\n",
        "\n",
        "fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
        "\n",
        "## Creamos un loop para iterar sobre cada feature y graficar la regresión lineal entre cada feature y el target:\n",
        "\n",
        "for i, feature in enumerate(data.columns[:-1]):\n",
        "    sns.regplot(x=feature,\n",
        "                y='target',\n",
        "                data=data,\n",
        "                ax=axes[i],\n",
        "                scatter_kws={'alpha': 0.4},\n",
        "                line_kws={'color': 'red'},\n",
        "                ci=95)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba9d4947",
      "metadata": {},
      "source": [
        "Ahora agregamos _outliers_ a un nuevo dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fcec748",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = generate_dataset(\n",
        "    n_samples=1000,\n",
        "    n_features=1,\n",
        "    n_informative=1,\n",
        "    n_targets=1,\n",
        "    noise=0,\n",
        "    output='dataframe'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f706cc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "do1 = generate_outliers(\n",
        "    df=data,\n",
        "    columns=['x0'],\n",
        "    percentage=0.01,\n",
        "    extreme_outliers=False,\n",
        "    only_tails=False,\n",
        ")\n",
        "do2 = generate_outliers(\n",
        "    df=data,\n",
        "    columns=['x0'],\n",
        "    percentage=0.01,\n",
        "    extreme_outliers=False,\n",
        "    only_tails=True,\n",
        "    two_tailed=True,\n",
        ")\n",
        "do3 = generate_outliers(\n",
        "    df=data,\n",
        "    columns=['x0'],\n",
        "    percentage=0.01,\n",
        "    extreme_outliers=False,\n",
        "    only_tails=True,\n",
        "    two_tailed=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37444567",
      "metadata": {},
      "outputs": [],
      "source": [
        "## vamos a visualizar estas los distintos datasets\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "sns.regplot(x='x0',\n",
        "            y='target',\n",
        "            data=data,\n",
        "            ax=axes[0],\n",
        "            scatter_kws={'alpha': 0.4},\n",
        "            line_kws={'color': 'red'},\n",
        "            ci=95)\n",
        "axes[0].set_title('Original')\n",
        "\n",
        "sns.regplot(x='x0',\n",
        "            y='target',\n",
        "            data=do1,\n",
        "            ax=axes[1],\n",
        "            scatter_kws={'alpha': 0.4},\n",
        "            line_kws={'color': 'red'},\n",
        "            ci=95)\n",
        "axes[1].set_title('Outliers')\n",
        "\n",
        "sns.regplot(x='x0',\n",
        "            y='target',\n",
        "            data=do2,\n",
        "            ax=axes[2],\n",
        "            scatter_kws={'alpha': 0.4},\n",
        "            line_kws={'color': 'red'},\n",
        "            ci=95)\n",
        "axes[2].set_title('Outliers (two-tailed)')\n",
        "\n",
        "sns.regplot(x='x0',\n",
        "            y='target',\n",
        "            data=do3,\n",
        "            ax=axes[3],\n",
        "            scatter_kws={'alpha': 0.4},\n",
        "            line_kws={'color': 'red'},\n",
        "            ci=95)\n",
        "axes[3].set_title('Outliers (one-tailed)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fcda0e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "## y si lo queremos con mucho mas outliers?\n",
        "\n",
        "doe = generate_outliers(\n",
        "    df=data,\n",
        "    columns=['x0'],\n",
        "    percentage=0.1,\n",
        "    extreme_outliers=True)\n",
        "\n",
        "## vamos a visualizar este caso\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
        "\n",
        "sns.regplot(x='x0',\n",
        "            y='target',\n",
        "            data=data,\n",
        "            ax=axes[0],\n",
        "            scatter_kws={'alpha': 0.4},\n",
        "            line_kws={'color': 'red'},\n",
        "            ci=95)\n",
        "axes[0].set_title('Original')\n",
        "\n",
        "sns.regplot(x='x0',\n",
        "            y='target',\n",
        "            data=doe,\n",
        "            ax=axes[1],\n",
        "            scatter_kws={'alpha': 0.4},\n",
        "            line_kws={'color': 'red'},\n",
        "            ci=95)\n",
        "axes[1].set_title('Outliers')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2734dfde",
      "metadata": {},
      "source": [
        "## Ejercicio 4"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "44621118",
      "metadata": {},
      "source": [
        "Utilizando la funcion `generate_data` generar un problema de regresión multivariada en el cual cuente con N variables informativas y M variables no informativas.\n",
        "\n",
        "Ejemplo:\n",
        "```python\n",
        "data = generate_dataset(n_samples=1000,\n",
        "                    n_features=10,\n",
        "                    n_informative=5,\n",
        "                    n_targets=1,\n",
        "                    noise=20.0,\n",
        "                    random_state=42,\n",
        "                    output='dataframe')\n",
        "\n",
        "```\n",
        "\n",
        "Dado un valor de _noise_ fijo, sin fijar _random_state_ (para poder asegurarnos\n",
        "que los datos que generamos son distintos) realizaremos 100 simulaciones de este dataset.\n",
        "\n",
        "En la simulación deberemos generar el dataset, hacer una división de train-test, ajustar\n",
        "un modelo de regresión lineal multivariada y validar el mismo.\n",
        "\n",
        "En cada iteración de esta simulación debemos guardar:\n",
        "\n",
        "- Los coeficientes de la regresión.\n",
        "- El RMSE de train y test.\n",
        "- El MAE de train y test. \n",
        "\n",
        "\n",
        "> Qué pasa con los coeficientes de las variables no informativas? La regresión se ve afectada por estas variables?\n",
        "> ***HINT:*** Utilice las distribuciones de los coeficientes para analizar y test de hipótesis para sacar conclusiones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaa01488",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4df91762",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Parámetros de la simulación\n",
        "n_exp = 1000\n",
        "n_samples = 1000\n",
        "n_features = 10\n",
        "n_informative = 2\n",
        "n_targets = 1\n",
        "\n",
        "noise = np.linspace(0, 100, 100)\n",
        "bias = np.linspace(0, 100, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08e0cf86",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Esqueleto de la simulación\n",
        "for _ in tqdm(range(n_exp)):\n",
        "    for b in bias:\n",
        "        for n in noise:\n",
        "            data = generate_dataset(\n",
        "                n_samples=n_samples,\n",
        "                n_features=n_features,\n",
        "                n_informative=n_informative,\n",
        "                n_targets=n_targets,\n",
        "                noise=n,\n",
        "                bias=b,\n",
        "                output='dataframe'\n",
        "            )\n",
        "            ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5772f3b8",
      "metadata": {},
      "source": [
        "## Ejercicio 5"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ad2f9ee6",
      "metadata": {},
      "source": [
        "\n",
        "Utilizando la funcion `generate_outliers` generar puntos extremos dentro de los datos que generamos anteriormente. En este ejercicio dejar setteado `extreme_outliers` como `False` y observe como variando el porcentaje de los mismos la regresión comienza a afectarse.\n",
        "\n",
        "Pasos:\n",
        "\n",
        "1. Generamos un dataset de regresion lineal simple (1 feature y 1 target value) con `noise` fijo en 0.5.\n",
        "2. Generamos outliers fijando `extreme_outliers`.\n",
        "2. Probar los distintos regresores a ver como se comportan frente a estos datasets anómalos.\n",
        "3. Simular con multiples porcentajes de outliers (desde 1% hasta 10%). Qué pasa con los modelos?\n",
        "\n",
        "Los modelos a utilizar en este problema son:\n",
        "\n",
        "    - Regresion Lineal simple\n",
        "    - Regresion de Huber\n",
        "    - Regresión Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82a019af",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression, HuberRegressor, RidgeCV"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be7e8c85",
      "metadata": {},
      "source": [
        "## Problema de Clasificación"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53495bd5",
      "metadata": {},
      "source": [
        "### Ejercicio 6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9639ca5",
      "metadata": {},
      "source": [
        "En este ejercicio vamos a jugar un poco con descenso de gradiente. Para esto consideremos lo visto en clase que es el problema de regresión.\n",
        "\n",
        "Como paso inicial, vamos a sacarnos de encima la parte teórica. Recordemos que partimos del siguiente modelo\n",
        "\n",
        "$$\n",
        "y = \\beta_0 + \\beta_1 \\cdot x\n",
        "$$\n",
        "\n",
        "En este caso nuestra función objetivo a optimizar será:\n",
        "\n",
        "$$\n",
        "MSE = ||y-\\hat{y}||^2\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6deb44f7",
      "metadata": {},
      "source": [
        "Para calcular el gradiente de la función de error cuadrático medio (MSE) con respecto a los parámetros $\\beta_0$ y $\\beta_1$, es útil primero expresar la función de coste de forma más explicita. Dado que $\\hat{y} = \\beta_0 + \\beta_1 \\cdot x$, podemos reescribir la función MSE como sigue:\n",
        "\n",
        "$$\n",
        "MSE(\\beta_0, \\beta_1) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\beta_0 - \\beta_1 \\cdot x_i)^2\n",
        "$$\n",
        "\n",
        "Aquí, $N$ es el número de observaciones en el conjunto de datos y $y_i$ y $x_i$ son el valor observado y el valor de la característica correspondiente para la i-ésima observación.\n",
        "\n",
        "El gradiente de la función de coste está compuesto por las derivadas parciales de la función de coste con respecto a cada uno de los parámetros. Así, el gradiente es un vector de la forma:\n",
        "\n",
        "$$\n",
        "\\nabla MSE(\\beta_0, \\beta_1) = \\left[ \\frac{\\partial MSE}{\\partial \\beta_0}, \\frac{\\partial MSE}{\\partial \\beta_1} \\right]\n",
        "$$\n",
        "\n",
        "Las derivadas parciales se pueden calcular como sigue:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial MSE}{\\partial \\beta_0} = \\frac{-2}{N} \\sum_{i=1}^{N} (y_i - \\beta_0 - \\beta_1 \\cdot x_i)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial MSE}{\\partial \\beta_1} = \\frac{-2}{N} \\sum_{i=1}^{N} x_i \\cdot (y_i - \\beta_0 - \\beta_1 \\cdot x_i)\n",
        "$$\n",
        "\n",
        "Así que finalmente tenemos:\n",
        "\n",
        "$$\n",
        "\\nabla MSE(\\beta_0, \\beta_1) = \\left[ \\frac{-2}{N} \\sum_{i=1}^{N} (y_i - \\beta_0 - \\beta_1 \\cdot x_i), \\frac{-2}{N} \\sum_{i=1}^{N} x_i \\cdot (y_i - \\beta_0 - \\beta_1 \\cdot x_i) \\right]\n",
        "$$\n",
        "\n",
        "El cálculo del gradiente se usa en el descenso de gradiente para actualizar los parámetros $\\beta_0$ y $\\beta_1$ en cada iteración, en dirección opuesta al gradiente, para minimizar la función de coste.\n",
        "\n",
        "Estos cálculos se pueden implementar en código Python de la siguiente manera:\n",
        "\n",
        "```python\n",
        "def gradient(X, y, beta0, beta1):\n",
        "    N = len(y)\n",
        "    y_hat = beta0 + beta1 * X\n",
        "\n",
        "    d_beta0 = (-2/N) * np.sum(y - y_hat)\n",
        "    d_beta1 = (-2/N) * np.sum(X * (y - y_hat))\n",
        "\n",
        "    return d_beta0, d_beta1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ce7dee2",
      "metadata": {},
      "source": [
        "Ahora, si quisieramos realizar esto de manera matricial, podemos hacer lo siguiente:\n",
        "\n",
        "Primero, necesitamos cambiar la representación de nuestros datos. Podemos agregar un vector de unos a nuestra matriz de características para representar el término de intersección $\\beta_0$. De esta manera, $X$ toma esta forma:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & x_1 \\\\\n",
        "1 & x_2 \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "1 & x_N \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Y nuestro vector de parámetros $\\theta$ se verá así:\n",
        "\n",
        "$$\n",
        "\\theta = \\begin{bmatrix}\n",
        "\\beta_0 \\\\\n",
        "\\beta_1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Entonces, nuestra predicción $\\hat{y}$ se calcula como $X\\theta$:\n",
        "\n",
        "$$\n",
        "\\hat{y} = X\\theta = \\begin{bmatrix}\n",
        "1 & x_1 \\\\\n",
        "1 & x_2 \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "1 & x_N \\\\\n",
        "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
        "\\beta_0 \\\\\n",
        "\\beta_1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Nuestra función de coste MSE se ve de la siguiente manera en forma matricial:\n",
        "\n",
        "$$\n",
        "MSE(\\theta) = \\frac{1}{N} (y - X\\theta)^T (y - X\\theta)\n",
        "$$\n",
        "\n",
        "Las derivadas parciales de esta función de coste con respecto a los parámetros se pueden calcular de la siguiente manera:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial MSE}{\\partial \\theta} = \\frac{-2}{N} X^T (y - X\\theta)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c4a59fb",
      "metadata": {},
      "source": [
        "\n",
        "Esto se puede implementar en Python de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "b7ac568e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient(X: np.ndarray, y: np.ndarray, theta: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Esta función calcula el gradiente de la función de coste del error cuadrático medio (MSE)\n",
        "    para una regresión lineal simple. La función toma como entrada la matriz de características X,\n",
        "    el vector de observaciones y y el vector de parámetros theta, y devuelve el gradiente, que\n",
        "    es un vector de las mismas dimensiones que theta.\n",
        "\n",
        "    Params:\n",
        "    X : numpy.ndarray\n",
        "        La matriz de características extendida que incluye un vector de unos. De tamaño (N, d),\n",
        "        donde N es el número de observaciones y d es el número de características (incluyendo el\n",
        "        término de intersección).\n",
        "\n",
        "    y : numpy.ndarray\n",
        "        El vector de observaciones. De tamaño (N,), donde N es el número de observaciones.\n",
        "\n",
        "    theta : numpy.ndarray\n",
        "        El vector de parámetros. De tamaño (d,), donde d es el número de características\n",
        "        (incluyendo el término de intersección).\n",
        "\n",
        "    Returns:\n",
        "    grad : numpy.ndarray\n",
        "        El gradiente de la función de coste. Un vector de las mismas dimensiones que theta.\n",
        "\n",
        "    Examples:\n",
        "    >>> X = np.array([[1, 1], [1, 2], [1, 3]])\n",
        "    >>> y = np.array([2, 3, 4])\n",
        "    >>> theta = np.array([0, 0])\n",
        "    >>> gradient(X, y, theta)\n",
        "    array([-4., -8.])\n",
        "    \"\"\"\n",
        "    N = len(y)\n",
        "    y_hat = X.dot(theta)\n",
        "\n",
        "    grad = (-2 / N) * X.T.dot(y - y_hat)\n",
        "\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1ef2b1d",
      "metadata": {},
      "source": [
        "\n",
        "Aquí, `X` es la matriz de características extendida que incluye un vector de unos, `y` es el vector de observaciones, y `theta` es el vector de parámetros. La función devuelve el gradiente, que es un vector de las mismas dimensiones que `theta`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0885fd5a",
      "metadata": {},
      "source": [
        "#### Gradiente Descendente"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91e4a496",
      "metadata": {},
      "source": [
        "Ahora que sabemos calcular el gradiente, vamos a:\n",
        "\n",
        "1. Crear una función _GD_ que compute el gradiente descendente. Debe tener condición de frenado\n",
        "por nr de épocas pero también por tolerancia.\n",
        "2. Generamos un dataset (con _generate_dataset_ de los ejercicios anteriores, utilizando un bias conocido y solo 1 feature)\n",
        "3. Inicializamos un vector $(\\beta_0, \\beta_1)$ al azar.\n",
        "4. Tratamos de calcular los mejores parámetros con el algoritmo.\n",
        "5. Guardamos la función de perdida en train y test en cada época."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "e2a249e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. C1. Crear una función _GD_ que compute el gradiente descendente. Debe tener condición de frenado por nr de épocas pero también por tolerancia.\n",
        "def GD(X_train, y_train, X_test, y_test, initial_theta, learning_rate, max_epochs=100, tolerance=1e-5):\n",
        "    theta = initial_theta.copy()\n",
        "    train_loss = []\n",
        "    test_loss = []\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        # Computo gradientes\n",
        "        grad = gradient(X_train, y_train, theta)\n",
        "\n",
        "        # actualizo parametros con gd\n",
        "        theta -= learning_rate * grad\n",
        "\n",
        "        # Calculo losses\n",
        "        train_pred = X_train.dot(theta)\n",
        "        train_mse = np.mean((y_train - train_pred) ** 2)\n",
        "        train_loss.append(train_mse)\n",
        "\n",
        "        test_pred = X_test.dot(theta)\n",
        "        test_mse = np.mean((y_test - test_pred) ** 2)\n",
        "        test_loss.append(test_mse)\n",
        "\n",
        "        # chequear que se cumpla la condicion de frenado por tolerancia\n",
        "        if epoch > 0 and abs(train_loss[-1] - train_loss[-2]) < tolerance:\n",
        "            break\n",
        "\n",
        "    return train_loss, test_loss, theta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "e867284e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Generamos un dataset (con _generate_dataset_ de los ejercicios anteriores, utilizando un bias conocido y solo 1 feature)\n",
        "data = generate_dataset(n_samples=1000,\n",
        "                    n_features=1,\n",
        "                    n_informative=1,\n",
        "                    n_targets=1,\n",
        "                    bias = 5,\n",
        "                    random_state=42,\n",
        "                    output='dataframe')\n",
        "\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "# print(\"Shapes - X_train:\", X_train.shape, \"y_train:\", y_train.shape, \"X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n",
        "\n",
        "\n",
        "X_train = X_train.values.reshape(-1, 1)\n",
        "y_train = y_train.values.ravel()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "d675279f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Inicializamos un vector $(\\beta_0, \\beta_1)$ al azar.\n",
        "initial_theta = np.random.rand(X_train.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "2642dc53",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Tratamos de calcular los mejores parámetros con el algoritmo.\n",
        "learning_rate = 0.1\n",
        "max_epochs = 1000\n",
        "tolerance = 1e-5\n",
        "train_loss, test_loss, optimized_theta = GD(X_train, y_train, X_test, y_test, initial_theta, learning_rate, max_epochs, tolerance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "id": "6bf5fea5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final optimized parameters: [16.49944286]\n",
            "Train Loss: [290.37119781912554, 233.35463377641568, 196.13404154720584, 171.83632194320853, 155.9746942098605, 145.6201737671303, 138.86071027439542, 134.44811125908373, 131.56755275161385, 129.68711510337877, 128.4595594957665, 127.65820733649943, 127.13508212801203, 126.793584347554, 126.57065352302925, 126.42512358003337, 126.33012117579638, 126.26810330768615, 126.22761784844086, 126.20118881338621, 126.18393585622725, 126.17267307156206, 126.16532069032993, 126.16052103275058, 126.15738780119587, 126.15534241778816, 126.15400718507027, 126.15313554095498, 126.15256652893662, 126.15219507610247, 126.1519525905247, 126.15179429517735, 126.15169095947702, 126.151623501607, 126.15157946489744, 126.151550717595, 126.15153195126561, 126.15151970054677, 126.15151170323902]\n",
            "Test Loss: [304.6181352979458, 242.58684773310003, 201.22657865848302, 173.52672956228633, 154.87882112264566, 142.24861838508093, 133.6345136304289, 127.7130053536509, 123.60649692550238, 120.731093585677, 118.69674190030906, 117.24163493635224, 116.1890640844368, 115.41898704915737, 114.84925344054619, 114.42317595542752, 114.10127716884674, 113.85578899238376, 113.66697074797726, 113.52063194491886, 113.40645563443903, 113.31685581134174, 113.24619267620426, 113.19022896997218, 113.14574970868587, 113.11029345616842, 113.08196034369443, 113.05927336937917, 113.04107704728548, 113.02646251383864, 113.014711582413, 113.00525452210181, 112.99763789027037, 112.99149981264158, 112.9865508397059, 112.98255902070834, 112.97933819734412, 112.97673877617581, 112.97464042367552]\n"
          ]
        }
      ],
      "source": [
        "# 5. Guardamos la función de perdida en train y test en cada época.\n",
        " # Store the loss function for train and test at each epoch\n",
        "print(\"Final optimized parameters:\", optimized_theta)\n",
        "print(\"Train Loss:\", train_loss)\n",
        "print(\"Test Loss:\", test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e02fba8",
      "metadata": {},
      "source": [
        "#### Gradiente Descendente Estocástico"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f767cd27",
      "metadata": {},
      "source": [
        "Ahora que sabemos calcular el gradiente, vamos a:\n",
        "\n",
        "1. Crear una función _SGD_ que compute el gradiente descendente estocástico.\n",
        "2. Generamos un dataset (con _generate_dataset_ de los ejercicios anteriores, utilizando un bias conocido y solo 1 feature)\n",
        "3. Inicializamos un vector $(\\beta_0, \\beta_1)$ al azar.\n",
        "4. Tratamos de calcular los mejores parámetros con el algoritmo.\n",
        "5. Guardamos la función de perdida en train y test en cada época."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "0e80946d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Crear una función _SGD_ que compute el gradiente descendente estocástico.\n",
        "def SGD(X_train, y_train, X_test, y_test, initial_theta, learning_rate, max_epochs=1000, tolerance=1e-5, batch_size=1):\n",
        "    theta = initial_theta.copy()\n",
        "    train_loss = []\n",
        "    test_loss = []\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "\n",
        "        indices = np.random.permutation(len(X_train))\n",
        "        X_train_shuffled = X_train[indices]\n",
        "        y_train_shuffled = y_train[indices]\n",
        "\n",
        "        for i in range(0, len(X_train), batch_size):\n",
        "            if i + batch_size <= len(X_train):\n",
        "                X_batch = X_train_shuffled[i:i+batch_size]\n",
        "                y_batch = y_train_shuffled[i:i+batch_size]\n",
        "            else:\n",
        "                X_batch = X_train_shuffled[i:]\n",
        "                y_batch = y_train_shuffled[i:]\n",
        "\n",
        "            grad = gradient(X_batch, y_batch, theta)\n",
        "\n",
        "            # actualizo params usando sgd\n",
        "            theta -= learning_rate * grad\n",
        "\n",
        "        # Calculo losses para cada epoch\n",
        "        train_pred = X_train.dot(theta)\n",
        "        train_mse = np.mean((y_train - train_pred) ** 2)\n",
        "        train_loss.append(train_mse)\n",
        "\n",
        "        test_pred = X_test.dot(theta)\n",
        "        test_mse = np.mean((y_test - test_pred) ** 2)\n",
        "        test_loss.append(test_mse)\n",
        "\n",
        "        # condicion de frenado\n",
        "        if epoch > 0 and abs(train_loss[-1] - train_loss[-2]) < tolerance:\n",
        "            break\n",
        "\n",
        "    return train_loss, test_loss, theta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "34060e38",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes - X_train: (800, 1) y_train: (800,) X_test: (200, 1) y_test: (200,)\n"
          ]
        }
      ],
      "source": [
        "# 2. Generamos un dataset (con _generate_dataset_ de los ejercicios anteriores, utilizando un bias conocido y solo 1 feature)\n",
        "data = generate_dataset(n_samples=1000,\n",
        "                    n_features=1,\n",
        "                    n_informative=1,\n",
        "                    n_targets=1,\n",
        "                    bias = 5,\n",
        "                    random_state=42,\n",
        "                    output='dataframe')\n",
        "\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "print(\"Shapes - X_train:\", X_train.shape, \"y_train:\", y_train.shape, \"X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n",
        "\n",
        "X_train = X_train.values.reshape(-1, 1)\n",
        "y_train = y_train.values.ravel()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "289cee18",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Inicializamos un vector $(\\beta_0, \\beta_1)$ al azar.\n",
        "initial_theta = np.random.rand(X_train.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "fc586dc9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Tratamos de calcular los mejores parámetros con el algoritmo.\n",
        "learning_rate = 0.1\n",
        "max_epochs = 1000\n",
        "tolerance = 1e-5\n",
        "batch_size = 2\n",
        "train_loss, test_loss, optimized_theta = SGD(X_train, y_train, X_test, y_test, initial_theta, learning_rate, max_epochs, tolerance, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "d28b6fb1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final optimized parameters: [14.32505616]\n",
            "Train Loss: [124.0118722961622, 125.01701091562647, 135.92265829697524, 131.4483799170779, 129.44370576923262, 125.60041637569222, 123.84193937240985, 123.92340824344633, 123.84082355038358, 124.98584044744099, 123.84641526743054, 126.05245564511817, 126.26609093519754, 130.1908614152607, 126.42883025885348, 140.4363760171986, 125.71531768585369, 132.72879566255565, 124.91788000563105, 151.3394222781235, 132.11795005779786, 126.00496227338073, 132.16585983588323, 131.06712461826453, 133.76403855021715, 134.50744569883972, 135.12987035479313, 126.38041210412199, 123.94050201241302, 125.99669453549446, 135.83035885456775, 136.74469907851747, 130.22386447337524, 124.01082389336003, 126.6167406491816, 126.33273405131567, 125.67909780699964, 150.06349655993517, 123.88162307945998, 128.0158523682577, 125.20116992547806, 136.23899446408657, 131.58651361959772, 129.47889482711503, 124.13048072403072, 123.90919152583184, 126.64192015801724, 146.5961303795341, 133.30750012157785, 126.02371730823043, 125.95178528978008, 132.76863346019323, 135.4033222869095, 125.07912223120982, 125.14418412111755, 123.93949334825103, 146.9557591459636, 146.92101623281897, 126.60385871334988, 131.52676018609773, 134.06381481206643, 144.52601435062272, 130.13547867754775, 123.84541036123355, 124.16043446672475, 125.01053116396855, 125.1620891456271, 129.61774125596884, 136.55980788837456, 154.75951985230205, 128.74475335793213, 130.19427802807277, 125.45835699965885, 129.37321103642594, 127.04738546902081, 154.7682329962919, 129.59355963145913, 128.06813105933048, 151.3558823652188, 123.83997903707964, 130.97742168090582, 142.82072956878872, 126.79070084829002, 125.26617446406046, 126.66117287140455, 132.11453172005426, 123.9478880871231, 174.3473424676366, 134.4299386979153, 123.85099489772685, 127.90226839585712, 123.98618286764159, 125.2304436927358, 129.01673535026387, 124.90361817775954, 124.77087747644498, 123.87371478968595, 137.82018359391034, 137.16727894463827, 124.5672494344337, 129.89941768196985, 124.03442875896378, 153.6584317763045, 124.39844817328168, 126.88738789934392, 150.2647382905196, 128.72154060632178, 129.40418842607994, 127.67739196648752, 127.25815743184859, 145.1487107964815, 128.5058453927034, 147.8065222121004, 134.2023753940536, 132.81629701702613, 137.28613348120663, 167.53063410192527, 126.81556594456855, 126.11433874816812, 124.38079647111545, 147.27896222905355, 124.16745094530306, 125.49897239435946, 124.14358827795415, 141.26726988872414, 123.84111403582563, 124.53541572434537, 125.26584535849119, 127.56089691047283, 141.89001821624583, 129.46804670428963, 128.58719363702718, 123.95446977171473, 126.95594192793706, 124.71189640910164, 127.65613981719657, 123.8487014115651, 126.91087112813588, 130.48598608208565, 129.85858496854652, 151.5672680267349, 129.94268638061803, 126.51316168922287, 141.680136225354, 126.94191714473509, 132.15208137239662, 146.45990715365764, 162.55001209701305, 131.59565233060744, 126.56007369415777, 128.99997789435756, 153.3573584757867, 129.90637467608346, 125.21249246585734, 126.2490256500294, 125.73827195734698, 124.54322053975932, 137.82596384334366, 124.23855819574982, 123.92827039651269, 144.61334623844425, 147.66882877168564, 143.3013045736959, 125.33679773853294, 123.84857770155507, 139.6889158212927, 140.84595987375835, 135.18534840368127, 127.07469338145887, 131.75137563138952, 134.7942966587594, 142.5741218919216, 134.00017088732358, 124.66782305094698, 145.3914939315686, 124.7125360703504, 129.09163077863533, 152.31092267961287, 128.27408499227386, 126.09532366838417, 132.14782948435223, 124.22499599834568, 128.65188987131182, 124.23861525310724, 165.74622044894812, 125.83120003608703, 143.18160376841345, 150.05758917975368, 135.536138164509, 127.0523260979321, 134.21551067391624, 146.72823121017444, 128.09098673034052, 151.58131583969367, 125.37943829258333, 124.04602589332242, 123.89449445172872, 134.25746260067694, 137.79263797314889, 137.2770137704632, 140.12310745933073, 123.90992981935018, 124.1758399621542, 123.84009107238938, 143.0929433322116, 124.31910119518052, 124.37868438930745, 123.89844114684183, 125.26421244735482, 140.94239568794342, 124.16662324453995, 169.81549805738084, 150.57625774975625, 130.88040064069526, 126.99088726597114, 124.74324940048085, 123.92519918983857, 140.5279511245685, 148.14171855206428, 159.48665537371323, 129.66699853222366, 123.9744885016332, 134.34422700071224, 135.74644689321977, 124.86475500103879, 124.87976748402687, 124.20075544472712, 126.96272918965133, 138.80946728421893, 125.41941010128875, 133.59979964441342, 127.43318858845255, 124.30962891179517, 125.65112199233299, 126.57359337065749, 125.32977491710288, 124.64834534061211, 126.78181230102888, 149.9097195306289, 127.16593336911481, 129.4076401648456, 136.52133225866007, 143.60195889985388, 127.48034414741782, 123.87447596363552, 129.51300221965886, 138.599650498865, 124.67241552076548, 128.44718758244437, 124.81629344462192, 137.83711685723017, 128.1766553805551, 130.4014532768584, 124.83610771606462, 128.2266398094702, 131.34615736350227, 125.65721354689973, 126.44449327824721, 139.51087890293928, 160.6361398587677, 138.36737068997573, 124.20508100166846, 136.62125269054584, 124.39391525256826, 125.4126271065135, 131.6089434133191, 124.26844703187564, 123.89789122332269, 123.84922917760431, 126.21849451760855, 146.96230139853967, 126.92547602621516, 124.74942905741736, 135.64871191565697, 125.20159032803393, 143.35404256282484, 125.46138540069725, 124.9663062069055, 148.91164532042512, 124.94762415837394, 124.61989506011604, 164.0519980115938, 123.83985500565562, 123.84390237373177, 123.90508755382572, 138.2590626466911, 125.88844030312937, 123.92230840048589, 145.95801230075705, 150.65385054178128, 126.62245966212669, 130.05666013091343, 126.25967793136736, 174.3826266029014, 123.88573212476014, 125.59718190928925, 130.62564059075817, 199.0042417386256, 125.01488325151001, 130.40352158661824, 123.91880962499876, 124.25477599040062, 129.2986251608333, 148.24583798202318, 138.4369077685946, 136.6052689520095, 124.49266198113392, 124.14693554087658, 129.49419657229225, 124.06593615962585, 133.7712003871907, 124.04824905221801, 137.4276279641662, 160.38734148021595, 128.74300744830208, 132.2329766976558, 123.83985743578322, 123.9675000434995, 123.9327636496409, 157.81955610717202, 123.87038629678304, 131.2184752046447, 132.346937292758, 128.44586890078438, 130.85654844538735, 125.81758062023054, 123.86531632834344, 135.72806531537458, 137.7939716646326, 134.49910663280207, 130.85323450190003, 132.60292561371435, 127.70878958570621, 153.84808220721519, 126.11846378307739, 124.01820994869881, 129.20031778109455, 124.54484372225492, 146.6068197446499, 131.27859655602464, 123.88497000349474, 138.5827131136937, 144.99724534386078, 123.9064706570951, 125.29798831479806, 123.88805821427694, 144.08020507863387, 159.44723525144087, 124.50789592998598, 131.6531404049965, 128.0902191361057, 127.11778061057441, 138.59124152413847, 129.84825505910635, 123.91795721809969, 132.3234964162266, 124.73413334165927, 126.45683026095166, 140.6186246978038, 142.4312028054388, 131.59762239137584, 127.29740157321721, 123.96165365325203, 141.48479374064883, 127.54912836411532, 123.84376121815076, 125.04564324784971, 125.71575784384635, 123.86322415927924, 150.46208093979766, 152.2757494199976, 140.7263141275487, 123.92724186148305, 142.1124785659893, 128.73142685511795, 171.74339375260388, 123.91817649363892, 130.30003853769063, 123.84761614480209, 123.93661160035985, 132.74271344852073, 126.05853356376988, 126.10211628220335, 125.67971865130829, 124.18679409179654, 128.05803674293267, 142.5339988216884, 132.8135157496389, 124.84489467924395, 126.30700277509841, 125.57851493714385, 134.0392454882588, 124.62930038700777, 124.22158274610935, 132.18863616418986, 135.98853400968122, 124.09385625738261, 126.73572735075882, 123.94257927091493, 124.77577152607684, 124.25580517725167, 125.96346787223742, 124.67273755724844, 128.07586749048292, 123.83997831642337, 133.62216082341615, 127.72387468152439, 123.86949074869213, 154.91107665384175, 149.69383694582183, 126.34774863980002, 129.11130133017647, 132.22920081452878, 124.32028575270448, 130.6135942704129, 127.61140114088396, 124.62564054501543, 133.95534595180533, 137.7782263415615, 137.0311694370604, 132.05515359477934, 124.27349353365946, 131.84037840772163, 130.07454851922762, 129.59097125668734, 123.84199746467216, 125.4786931192883, 141.93834941772118, 129.52451920958308, 130.556705158657, 124.0407127989586, 123.83987508331819, 152.07015449843144, 126.20013455875237, 125.30123511527226, 123.84020382496948, 124.96737243155032, 135.49785449073985, 124.4543194389154, 129.5478966432447, 127.51181849132828, 123.98631487651106, 125.83362128183872, 125.73888406614172, 123.87290462571953, 126.6171693334278, 123.90324435890652, 128.6429169178976, 125.6138052944413, 126.10454225573187, 139.93182234219876, 157.7289064505875, 125.47389037892277, 125.9754061224875, 123.8415576231767, 124.34776121359828, 132.8012197979643, 150.43360051057013, 131.05297950179175, 123.89453573415916, 126.52652274139231, 124.08079690478604, 136.54906622573913, 127.83665773205641, 127.42970613785114, 129.18694772032768, 129.51549563969746, 128.69155611312107, 186.87705846395008, 126.70025120189557, 123.90565661030628, 125.85693541959125, 124.04598311189757, 149.42015313341216, 123.90719425741419, 139.63168660314065, 129.47725839902373, 152.71518364825386, 128.67354859155006, 126.43628569299253, 143.30819319151888, 124.52114616144296, 136.61034803904954, 125.06723806459249, 162.62114175077593, 156.59378852520422, 126.26485154544498, 139.07037473974623, 140.47432672082365, 139.03056189272615, 124.19529159481831, 126.70501211561746, 137.7593977230274, 147.48855607411718, 133.0623134496209, 124.49984579985681, 125.54738551084866, 124.94726046971955, 143.51363317473337, 127.96397016823721, 124.623464368005, 124.74383511637119, 142.88181830973636, 156.02559674116972, 127.38847451337661, 123.91074020727217, 124.16608987622425, 124.51248502651833, 123.91000654133681, 130.71582630677318, 147.74184063872664, 162.54762621148836, 123.89244406539773, 123.87061496300721, 145.04964470467183, 136.51330079863286, 163.45691524026424, 125.89767338898328, 167.3502630106702, 124.24317529078013, 127.5312470450592, 129.6551822231179, 126.92224925924991, 129.9834818741827, 146.60541811981437, 125.09479040750882, 127.28905860312364, 128.2220533784493, 129.41986095086918, 143.7652527841902, 123.97279060132615, 123.94899579586905, 123.84149627885549, 125.34550127091953, 130.82627835351542, 137.02330191043245, 131.51604060154716, 142.73224519146336, 123.89228057486633, 127.79454445933371, 125.91601579899957, 125.10662806481847, 127.37561270820105, 147.17217341305982, 123.90282373243838, 124.00726130842848, 153.77159936300401, 157.51457400100458, 124.62868109967926, 123.85322248832989, 146.70778058304612, 189.19225009533795, 123.9702490303201, 124.37910490901814, 128.74895702166216, 141.32439528206206, 124.42724154609844, 134.2972943271189, 136.10775364754704, 127.7749563649464, 124.57552141159518, 143.24936459105822, 126.39130912844485, 124.8947196075401, 130.51938051789577, 158.1134869915749, 131.070416557394, 131.1598876185815, 236.95984941397808, 128.22140141285618, 123.84078143826048, 130.10126038932555, 168.7570011071786, 128.12406138819063, 125.93164426721691, 128.84069078599435, 150.87653466842914, 132.50328801147242, 146.2679768395808, 132.09370550586252, 132.4545072437407, 126.03594195367226, 127.67904634491246, 125.12442928449316, 127.53568999250824, 143.63994352712237, 125.91328264311728, 129.33383125239132, 127.81452800585822, 125.00132991281593, 137.8404316903455, 126.63903568931099, 148.3797458459938, 126.44421361843253, 126.4590108277835, 125.43521337065872, 124.36997760552036, 124.20829407086202, 140.53205218682714, 125.56003146812476, 125.32780502948619, 132.33521911878327, 124.35738076266774, 165.83743541402532, 129.5004270239056, 124.63675414150417, 127.8599632135421, 123.99062796875366, 127.0388192035466, 131.20972114956916, 123.86229428165286, 123.97979013641432, 132.7284457003587, 157.24642839685896, 149.2093027405912, 132.54818029468234, 128.97624699909258, 123.89559599299535, 161.19412540384468, 126.59138394821503, 157.2804957275031, 148.67203746300228, 130.88915397669334, 123.86222534549967, 132.42653129528773, 136.3654904874714, 126.05185951364528, 129.0946039390627, 127.74363173763724, 124.02764647021418, 128.66757537854954, 132.29769393814541, 149.5824285092761, 126.16432048511699, 141.42614948847518, 134.21292888429215, 127.37599929892721, 131.63257674705278, 125.59986596262657, 126.10592997892932, 132.542586792501, 124.55052961109584, 126.10555472251582, 123.88980263410907, 132.5603223582259, 123.84747984178382, 157.17175374507204, 155.9872131143189, 123.86208504518196, 129.527807898866, 125.80671794433233, 149.2007940612033, 123.8782773422535, 125.93246423993065, 163.21300930914043, 126.20801499435733, 124.32984793389772, 134.40726223423297, 123.93284980014381, 123.98392094351195, 123.84055501779925, 123.99544064413185, 127.31952784584519, 123.84913569090737, 126.0738971686495, 133.98664429415805, 126.15741817583144, 129.5120284334359, 144.07500471583487, 125.14914429877341, 149.06868311638885, 124.28200481473628, 181.4048384786837, 144.2221845909649, 127.54841482584659, 125.54004141703543, 124.47382720488659, 125.73014782957286, 125.40971958022415, 123.87572445786769, 125.1185729437564, 130.88540434910848, 145.22151411832766, 123.96886226593443, 134.28172998037317, 130.1228537437162, 127.83086387978959, 123.87952636009987, 146.107964480474, 124.02290265540914, 129.0342962445857, 124.22129893890619, 137.44841930869265, 134.9125654968686, 124.07210908525776, 125.58742520183802, 123.84114209350997, 151.7388022046397, 134.66619509573454, 133.5295509906884, 125.86025670292045, 134.35918039327743, 153.83810384385765, 129.44803298219327, 125.99541586584924, 131.21631794396126, 125.89740631414992, 123.84184647148328, 129.4395559558065, 147.2507636243455, 128.95010964758202, 124.87036049945652, 124.54275192306984, 128.0405136539965, 132.73183783984769, 124.16529175879901, 123.93611511908304, 127.75676040427061, 124.02628778011393, 124.27945547704172, 142.81027314268277, 135.25087114822128, 128.89080662949766, 123.84995642999328, 124.15901550618685, 123.85953201287235, 124.48082563830023, 125.8763795478036, 130.4532669979411, 165.83431205943234, 131.51780008111197, 135.20603613110532, 124.49154320565977, 125.26670025573884, 123.9254844769589, 140.1407467375979, 124.08043520959058, 123.85767876394883, 130.02386610361071, 125.5663197642614, 124.01041457861105, 124.83706893698755, 123.86211110521646, 128.57682925964056, 124.29744676525162, 152.7986073903967, 132.90235101089002, 141.21231060621517, 135.57277045692265, 162.81265787559676, 133.53821345062093, 135.15765967371507, 125.05750594675278, 128.27402109170208, 128.5634393001155, 129.61971238296132, 137.84894976033124, 208.4707680232212, 131.96121674258367, 134.7403376676771, 123.84176750009817, 125.42085230645283, 127.1712186657839, 125.82903071998363, 131.1496279931943, 144.3523952931013, 131.91157877740076, 143.00287387364767, 130.39171623211752, 129.0122789400084, 125.52382147753211, 132.22953958671698, 125.38679636387853, 125.46962193383938, 134.34703541051624, 123.89796483556846, 124.15482611402359, 123.87263688803685, 124.09004951356647, 132.5986179973005, 128.71011015027338, 133.62925388593692, 175.0008667067584, 125.13621282335198, 123.83994107026709, 132.23358219986574, 133.46040693791758, 124.61782001575176, 123.91384670439355, 124.81317526629857, 125.3873297765227, 126.95737597678841, 125.64027960432891, 131.8770421433329, 125.83603594542845, 130.03979971280776, 126.13949740399704, 125.28695705974563, 123.85890220020755, 134.8959695217723, 124.78089429794181, 130.4292287087442, 151.611132979261, 126.29121305297, 186.77546694234223, 130.41459927095846, 124.99774439076417, 128.93458035745505, 123.89157257325446, 124.68923678765346, 125.65101218058959, 142.21216415438036, 143.9320751395649, 123.83984542908932, 127.67111870571426, 124.31274202858873, 124.85969575347431, 124.02947590968024, 123.98382500571803, 124.8565180102691, 125.16042821368187, 124.03516243607956, 124.27660972585062, 127.6806756003055, 124.4889806942541, 142.00896484206314, 126.00673511189736, 126.78285722868195, 126.62815905853574, 123.88879920910684, 123.86498614110562, 123.90000453593467, 123.84030143632194, 131.0138101139098, 124.20175134997173, 123.84539038447804, 139.7406514269234, 123.94307289072003, 124.16575157295945, 125.33485752794765, 124.02961115476506, 143.02229792038142, 123.87979028097394, 139.3403075309235, 124.9408463929541, 168.48034688258966, 124.14643080026988, 124.27062269478523, 129.2961367613582, 127.44076271367294, 219.4496880498405, 124.30437126013588, 123.9171355345012, 131.98606019543234, 130.5479705969757, 127.56728150728108, 124.0717108123311, 128.96221379605163, 125.59062823484675, 138.97172918596956, 136.879892501919, 126.71086063363637, 128.05460922357994, 130.44410570023453, 136.81303614108194, 127.26680170941682, 127.85541837565655, 124.07371880074868, 140.80171040296213, 134.40595277831827, 124.21032293015774, 128.4953683893184, 168.13792231306255, 132.15540345544022, 128.05036933336007, 126.86537810188572, 130.21905793001625, 127.34103797403914, 150.93013172995686, 125.70717146673108, 164.26026816483107, 136.872931046509, 126.86326994797564, 123.85984991601656, 125.1536883747554, 127.1042693032246, 135.9035633660943, 128.4323180279072, 125.65835496078645, 123.96349319349541, 125.37073412287016, 141.86380069483457, 151.352170248165, 145.77065737191506, 124.56096803557077, 172.92263688526552, 130.0582417488918, 132.26899330423166, 137.26928162209083, 126.78540500875219, 134.99729299810465, 128.16813011210036, 137.37959858815827, 127.35780434175322, 131.8454854885851, 130.98250939427257, 127.32102213458046, 123.88926560492601, 132.6870649901903, 125.83093460562284, 126.40269806452942, 133.79249072845545, 124.8804659203258, 124.23242972244064, 134.384130910413, 138.67649869823737, 132.8975555192913, 124.98715449508654, 160.3519401488968, 179.20563008679295, 133.92556309772638, 123.93322166443568, 138.42364561747607, 153.415053677804, 129.20524824586985, 128.40826830820714, 123.90307100366252, 126.53147797941693, 142.24480165664542, 126.14546525802733, 133.14833554649886, 131.9292297882105, 123.84278350820368, 133.20016340336915, 134.3875369950667, 143.04253980278298, 124.782349105828, 147.0123682445249, 209.25058992305117, 124.14522779361764, 133.182331900621, 147.46138346351523, 125.83336339938631, 165.15710590637937, 125.50724159636701, 135.1265148770695, 142.9592438375962, 173.75243100699538, 125.50099912633348, 128.36238894684405, 124.02256186984951, 137.4829823132343, 124.18317742866448, 125.60678762758977, 123.84127161005706, 128.05278766665043, 147.16887732011722, 135.26932516058253, 124.75457723157895, 143.72185235983258, 128.84266761622916, 125.20999295940878, 147.7853335880977, 147.43451811333787, 125.80560982629324, 159.26212365471787, 130.71244297573122, 125.01425348922125, 124.15071233430514, 164.1915412681514, 128.79198117547853, 133.77968083559702, 130.95654676715253, 126.96658824241742, 126.08627338154255, 129.86109519380074, 135.05344572249803, 124.32964113270107, 126.32658883706863, 125.7216170362927, 127.19935249040624, 126.27499295953042, 123.95324644200805, 125.72278745195261, 124.95029709959479, 125.83850071411955, 124.27063378588667, 128.21133023503123, 131.56104709166527, 130.8144164082478, 129.69297114990414, 141.7798294623608, 130.86131381658961, 132.58673621842837, 124.43343577045144, 127.15030644062466, 132.0061647460794, 139.53383216921665, 132.099041159966, 129.83315415952663]\n",
            "Test Loss: [122.29265927660501, 122.45125668569388, 130.38307385456727, 127.01105584328731, 125.53136732250736, 124.01199397973878, 121.98728451400191, 122.16339975520837, 121.97988314515833, 123.39000646628845, 121.93423264377199, 123.12582498704089, 123.27077141885495, 128.33495138181112, 124.82203735880731, 137.4870074956437, 124.12590311197845, 127.96811605111242, 123.31949064261815, 147.01013957650534, 127.51056736499213, 124.41060062226786, 127.54639284912083, 126.7276817903119, 128.7470774586253, 132.23173603811213, 132.7875269020443, 124.77530816101932, 122.19012376160599, 124.40251800767626, 133.41166141933894, 134.2243634304765, 126.1039930699375, 122.29122752405378, 125.00281197742234, 124.72922996763118, 124.09006066126928, 141.31915815812835, 121.90810495885054, 124.49764230593073, 122.56666944217245, 133.77513683460631, 127.11392028928095, 127.68219832773624, 122.44619910067904, 122.14014947178107, 123.52894889672093, 142.88301068368781, 131.15663430067795, 124.4289268358598, 123.05804766343745, 130.6720895067685, 133.03133733729857, 123.48611646819816, 123.55272794786846, 121.90608195308279, 143.19665457425654, 138.8669730417473, 123.50263674627537, 129.55056477996928, 128.9733866401729, 141.07490038705484, 128.2843101080496, 121.93629187275523, 121.9763478062308, 123.41551922219746, 122.54195983908761, 127.80980621799658, 130.86850607912712, 144.99861534414725, 127.00464846814474, 128.33807478435722, 123.87027900720787, 125.47986326574478, 123.81126548901786, 145.00545677454508, 125.64099009850406, 124.53508832046559, 147.02442814672483, 121.96000973079691, 129.05198156124388, 135.68279887366364, 125.16940233009176, 122.60800597500565, 123.54227163874937, 130.0822774110119, 121.90718177267725, 160.47483013956332, 132.16244162197478, 121.92703405756657, 124.41640914334063, 122.25703766376823, 122.58524909747184, 125.22009075403555, 123.30463536002749, 122.30182083299238, 121.91068720660247, 135.17772626089538, 134.59927380056473, 122.94711439291757, 125.86529237704839, 121.92831236682451, 144.13446216331545, 122.09173366057186, 123.6994433361391, 141.47649125024876, 125.00588435754248, 127.61347256202328, 124.2561090895426, 123.95932949871721, 137.48826343672184, 124.84993364497724, 139.55703635538157, 129.07809451397927, 130.71499574174467, 134.7046460065096, 160.98581370185485, 125.19315803066989, 124.51730724923218, 122.08248993791372, 139.14582295951334, 121.97938189130245, 122.75819877875956, 121.96918898004455, 138.21791447921086, 121.98206567992743, 122.16603451535588, 122.60779598067427, 124.17335912929953, 138.7650229176976, 127.67222156908956, 124.90869080754754, 121.90820085215229, 123.74729216115152, 122.26703624449918, 125.98922766881422, 121.93027979268291, 123.7158230217552, 128.6044547716698, 125.83530575881215, 142.49561414500306, 128.1078519721431, 124.90327777565636, 138.5806981821515, 125.31366714179761, 130.11618865782538, 138.5079471463834, 151.13316147914347, 129.61297630410797, 123.47241151450659, 125.20790795261394, 148.7603460331289, 125.87040268583738, 122.57384864978444, 123.25914088543044, 122.91555594788878, 122.92092437111474, 131.83607057617107, 122.01164453936681, 122.17112289562424, 141.15127693807707, 139.4496834863624, 140.00284162833702, 123.7481476931644, 121.93047281435669, 136.8285198879027, 137.84744105319226, 132.83700884478415, 125.43995216880447, 127.23681796044606, 129.52610905554548, 135.49194999910281, 131.7778518258138, 122.24135324428725, 141.83141861840005, 123.10341405208563, 127.32540952912026, 147.8531200789877, 124.6829444609587, 124.4987851543196, 130.11234909135453, 122.56068056248989, 124.95547154592549, 122.57678917874948, 159.45190158541266, 124.24020188547586, 139.8979586672324, 145.8967530953659, 130.0891132615838, 123.81472662214843, 131.9706319709184, 138.71684149478256, 124.55147039360853, 147.22009998648977, 123.79108633639724, 122.33843949689148, 121.9055851045483, 129.1197409377353, 131.810557687391, 134.69656193589014, 133.5999713815384, 121.9044711502585, 121.9830479685231, 121.97230015897684, 135.8935596796926, 122.67042911700872, 122.7383017686373, 121.90513151089507, 122.60675416471474, 134.23135931607868, 122.49059133549015, 156.88018579346243, 146.3474224957925, 128.96374624477082, 125.3602840127769, 122.28547037962333, 121.90480601571142, 137.56761726995506, 144.2300616034525, 148.71703983826563, 127.85503889234592, 122.24040318049467, 132.0857891591061, 133.3369680142157, 123.26404853047389, 122.36715066823459, 122.53179134169297, 125.33348500020774, 136.05248026522847, 122.70651575770795, 128.6232245186918, 125.7792824639777, 122.6595340805397, 124.06233618374863, 123.48173923298498, 123.74106525034618, 122.23009352881644, 123.6259489773748, 145.76822886935454, 123.894442295618, 127.61664901194234, 130.83916304656327, 136.28792537596598, 125.82375277815338, 122.07693187593895, 125.58203675320999, 132.42902321763245, 122.24401635022659, 126.72850264080695, 122.32890294379732, 131.84460938681522, 126.47657671301576, 126.23496088956708, 122.34079438587861, 124.64883677147124, 126.93499879303668, 122.86195445200298, 124.83714044680943, 136.6715317572622, 149.6231119428869, 132.25087579181496, 122.536967799376, 134.11476211880435, 122.75548331070502, 122.70212571180186, 129.625014268224, 122.6117912677384, 122.12079482967634, 122.01617003653553, 124.61855729796943, 143.20235906964507, 123.72601581088534, 123.14261018950116, 130.174686883025, 123.61123647809447, 136.09580731169092, 122.73373976984897, 123.36978256068025, 144.90024163238718, 122.40850886250286, 122.21375245477839, 152.3194303562267, 121.9664991391719, 121.9399261309885, 121.90463967560636, 135.56604204424448, 124.29645963823972, 121.904652692554, 142.32615713832493, 141.78079792345466, 125.00829997130226, 125.98088334839107, 123.26639973728703, 160.50284369630967, 121.90710446052452, 122.82244966886914, 126.4005928423659, 187.8863417780271, 123.42001057536515, 126.23648747567246, 121.90452014546385, 122.01934207474986, 127.51627886125854, 144.32072378716927, 135.72328454355042, 134.10056831307847, 122.14239560734573, 121.97059679470367, 125.56828213915598, 122.36445775747357, 128.75248043016484, 121.93283068506774, 131.53127925043293, 149.4269413358136, 127.00303097952583, 130.18922325696528, 121.96666401083861, 122.23031706755808, 121.9053800730088, 147.40420557595496, 121.91209074177304, 129.27096983995318, 127.68189287158931, 126.72727672282761, 126.57152245812878, 124.22679724129804, 122.05751537510324, 130.23502887401168, 135.1545213534027, 132.22428150305208, 126.5690670020119, 130.52284641923626, 124.27844670671975, 149.18552198029624, 124.52132377697357, 122.30127815582512, 125.35372850723994, 122.92269692722563, 138.62231163813397, 129.32553671837562, 121.9072746950955, 132.41602943070978, 137.37059531932306, 122.13556888880737, 122.62833883694454, 122.10312413295853, 136.65875302704563, 154.02605826710766, 122.15077602140354, 129.6650377005419, 124.55092010284918, 125.48085904900132, 132.42257206688748, 125.82772170458574, 122.15461045299439, 130.27091135984782, 123.1263815229716, 124.84903180015722, 133.9817125775304, 139.24001385495168, 129.61476068000948, 125.6510211927804, 122.22178756612452, 138.4090837485596, 124.16501117482578, 121.99631244940285, 123.45170798079545, 122.90063906433288, 122.05279278661627, 146.2482335531094, 143.05049478052726, 134.06472885243335, 122.16949782716549, 138.96032422300064, 125.01304407177795, 164.60240577411824, 121.90450263777112, 126.16014349480548, 122.0109377641959, 122.18414210292939, 130.64875259001496, 124.46291483365006, 124.50540302593308, 124.09067552676875, 122.51501682031011, 124.52785523596096, 139.33018988691364, 130.7124923342937, 122.34608205598734, 124.70433539026959, 122.81020043372035, 131.8128446023137, 123.01427636745535, 122.00371276217422, 127.56342805294462, 130.43321460961633, 121.94922950199901, 125.11683288266835, 122.19329571202256, 122.3047271925568, 122.01983434721141, 123.06589462272997, 122.24420321128575, 126.38249322466584, 121.9600230832309, 128.64008137108212, 126.05286031941161, 121.91250654223913, 150.10597156913488, 145.58055663258727, 123.32654149069447, 127.34356449031674, 127.59377370921207, 122.05149556285568, 126.39168466403491, 125.94716085235989, 123.01033285746018, 128.89146434704438, 131.79952548437558, 131.22827634630391, 130.0286392377313, 122.61767625913275, 127.30322284175806, 125.99404476848433, 125.63909505602388, 121.98761451864117, 123.89062986853432, 135.00031884266826, 127.72414725041136, 126.34962800602821, 122.33141906640122, 121.96755585706876, 142.88943465624956, 123.22586861110193, 123.71225171256445, 121.95689934643245, 123.37088734172511, 130.06001974414622, 122.82302764235158, 127.74563474895797, 124.13856000511049, 121.91473081701179, 122.97896615621629, 124.14919313107737, 122.07372383675194, 125.00322337238467, 121.9047442841046, 126.91024960100393, 122.83337199916942, 124.50776622253261, 137.04261781763876, 147.33286330091153, 123.88582565089251, 124.38169512057053, 121.94797513818718, 122.70321321638932, 130.70142441231906, 146.22349006754382, 129.12065895011767, 121.90557967875743, 123.44928324843642, 122.38360503849924, 130.86031369558313, 126.15866498075606, 124.08042414622884, 127.41334927880041, 127.71585198081277, 126.95534999355824, 177.54965567590938, 125.08287184424137, 122.13418921218903, 124.26551133610457, 122.33838310435227, 145.34257924229578, 121.90454799234374, 136.77806288845676, 125.55589593004464, 143.39484540429302, 126.93865553762049, 123.38722197114542, 140.00887692971787, 122.1581038877962, 130.90705553522014, 123.47391295042723, 156.76231296542153, 146.43991371353133, 123.26992644401564, 136.28285936032967, 133.87050490928632, 136.24771331595946, 122.52523885154602, 123.5726405913189, 135.12391146448573, 139.30916123139244, 130.93630853179687, 122.14634150038083, 122.78981131415645, 122.4082859814508, 140.18884010710318, 124.46051570799084, 123.00798698992813, 122.28581599371644, 139.63520077696833, 145.99322745561648, 124.05127358659331, 121.90445717133545, 121.9787910082662, 122.88726411171359, 122.14151259935846, 128.81394538032706, 143.8817725091304, 156.69898964252377, 122.11111581160067, 122.06896601530799, 141.53271016175142, 130.83303840047094, 157.48204027367134, 124.30552239328232, 160.83082099305724, 122.58216408427856, 125.87171686708534, 125.68612217317386, 125.29493063761744, 125.92706644296977, 138.6212204342523, 123.50218820374525, 123.98110499037611, 126.5189134720397, 127.62789433062775, 140.40918250229433, 121.9116651109142, 122.2030029706833, 121.94825916756335, 123.75692093798736, 128.91450057426803, 134.47158533721233, 129.54085134558514, 135.61431155580794, 121.90589838037782, 126.11917868771016, 123.03405311503415, 122.50708763082986, 124.04218615049898, 143.38533352708336, 122.12935317854426, 121.92018452338414, 144.22324042537505, 147.16419987611886, 123.0136092307937, 122.02787345397229, 138.7009177656428, 172.29080662560136, 122.23429831753388, 122.73877701413865, 125.02574206584475, 138.2681257086197, 122.10698945856544, 129.149860374967, 133.6584477183801, 124.32556783635773, 122.18853066029834, 139.9573336345121, 124.78583064973346, 123.29535602750703, 128.6349135278141, 147.63556423420948, 126.73012508682248, 126.79655476718355, 220.09525492748898, 126.51830564804, 121.9521592421081, 128.2530105008367, 156.04161348764467, 124.57518883704623, 123.04453104169146, 125.0922406687833, 146.60823220925474, 130.43305203141637, 138.35857233537388, 130.06346631668472, 127.76245762558911, 124.4408653996583, 124.25728572100097, 123.53253734305052, 125.87590135555149, 140.29945968086028, 123.03222163752062, 125.4511105987679, 124.35377913807196, 123.40601801532475, 131.84714729307262, 125.0242019667154, 140.00414342870312, 124.83687084186619, 124.85113318276731, 123.84709124212993, 122.72845057258108, 122.54080677926854, 137.57122694244438, 122.7980883652126, 122.64745371102326, 127.67311965248308, 122.07036372938921, 159.53034265877156, 125.57283882045294, 123.02230125991271, 126.18050599448956, 122.26328696403694, 123.80526556131586, 129.26302287510927, 122.05065084840473, 122.24798050286935, 130.63590550769487, 152.12564484258326, 140.6517199130488, 130.47351474493823, 125.19065999409113, 121.90544538827928, 150.06318037604456, 123.49402048955473, 146.98002803346034, 140.23224421420386, 126.59568494212702, 121.91661251541686, 130.36384846382563, 130.72034964676794, 123.12542259785434, 125.27673661088944, 126.071408123111, 122.31400169987472, 126.93311712414749, 130.24763012332016, 140.94319109309626, 123.20154241949095, 138.3575517567775, 129.08607226199095, 124.04245925350514, 129.64641713601964, 124.0114468455173, 123.16196876028692, 127.82846316431478, 122.92890214532831, 124.50875243255808, 122.10632561617845, 130.48445722797177, 122.010477802193, 146.89448104189063, 151.0370573394739, 121.91670685730507, 127.72717033950275, 124.21610052027752, 140.64507467848026, 121.90908188606413, 123.04508103319317, 157.27203322648825, 123.23122662590434, 122.05631713710883, 132.1421642621425, 121.905387967604, 122.25384261115867, 121.9775872115767, 121.9170092464193, 124.0025924379815, 122.01587642181912, 124.47789984190824, 128.91509859221392, 124.55922641016208, 125.58132444549555, 140.68032196928078, 123.55779291188375, 140.5419048698437, 122.03251035113713, 172.87509352204728, 136.76889716947235, 125.88788423109513, 123.95189066963542, 122.13210257269344, 124.14056213648108, 123.82151438901914, 121.90993815326024, 122.51457833702689, 126.59290593860044, 141.68290648611256, 122.23229220084718, 132.0298825143852, 126.02959690120522, 124.36543172824558, 122.08696570789404, 142.45705153353924, 121.92473359815607, 127.27247195408576, 122.55629295325413, 131.54717935632877, 132.59362047171672, 122.37243790548715, 122.81604522991456, 121.98226408963215, 147.35677195340358, 129.42905650909069, 128.5702799189006, 122.99674522566018, 132.09916379391416, 149.1768781097978, 125.53453019465347, 124.40126775359042, 126.83847596017748, 123.02158852145598, 121.94671280214351, 127.6460146080193, 139.12385083853516, 127.19468515176153, 123.26991247938123, 122.17012713118311, 126.34946047873434, 127.97039885032363, 122.48897066498193, 122.18337476604192, 124.31260313503515, 121.92576574321185, 122.03126506416884, 139.57247463144822, 129.87242568763148, 125.1286057846214, 122.01841858373919, 122.48131678083588, 122.04411087518668, 122.13591815816945, 123.00752064163734, 126.27321230422608, 159.52765676043032, 129.5424457201901, 132.85545831848845, 122.86422051867159, 122.60834148476047, 122.16671054462827, 137.22668200177534, 121.94419007462317, 122.03955311892952, 125.95676107697855, 122.80220714751229, 121.92107115927854, 122.34137239210675, 121.91668928378495, 126.84892986709703, 122.64547619192713, 148.27602462464168, 128.09841093973463, 138.16960245017245, 130.1169556562466, 151.3405252736685, 131.3637432544125, 132.81231386233947, 122.4764023010956, 126.5673461147481, 124.8915257662992, 125.66014051063887, 131.85366901498801, 187.7045206529363, 129.9437487710101, 129.48522232012522, 121.98628442756755, 122.70744950716569, 123.89815676016826, 124.23806727480574, 126.78893498059333, 136.8699329133149, 127.35637359020399, 139.74131874206702, 128.51843264456963, 125.21685064810929, 123.935712762507, 127.59402717032272, 122.68543174099176, 122.73909307733693, 129.18748030653904, 122.12092392547046, 121.97394439864294, 122.07317273023784, 122.39541969912788, 130.51896524361425, 124.99760757764199, 131.44541466144318, 167.39474385725848, 122.5256605549148, 121.9607731752738, 127.59705179590233, 131.29391954604077, 123.00189894411577, 121.90443907166375, 123.20992794735626, 123.7990211780322, 123.74829411946037, 122.85079377728151, 127.33058904304892, 122.98057678388706, 125.96848037345875, 124.54179554477896, 122.62128115228245, 121.91903887857576, 132.5788056380457, 122.30777254956398, 128.5526701633511, 142.52995803346718, 123.28790859911429, 170.36346748088647, 128.53931903310732, 122.43934301143965, 125.16038919068316, 122.10954277286346, 123.07856473725678, 124.06222728833849, 135.2119845099313, 140.55522425205544, 121.96535670875197, 124.25164777443226, 122.66311818612328, 123.25875311046511, 121.92675239218055, 122.25370686579629, 123.25542560578117, 122.54091213645182, 122.32404763043523, 122.02987793385951, 124.25844452416719, 122.14037779807099, 135.05489601358872, 123.09499902584314, 125.16190584328406, 123.51943172493029, 122.10448788891938, 121.91488260275406, 121.90498652848845, 121.9558962629313, 129.08506054572845, 121.99462041920316, 122.00297637991902, 133.3056162384792, 121.9065220105139, 121.97864431243255, 122.65198299554, 122.316635031992, 135.83885313873444, 121.90862076101972, 136.52107294266185, 122.40435730439589, 161.8016949418144, 122.46589536288442, 122.02696990528753, 127.5139865859465, 124.08824586621527, 205.25826497662925, 122.04353950506784, 121.90447831032945, 127.41200008575812, 126.34317260419166, 124.17788891216018, 121.9410049228307, 127.20587333791887, 124.0022619761418, 132.71461043495736, 131.11275518647938, 125.09303119566394, 126.36263256511384, 128.56624576478032, 131.06171824363125, 125.6220736326774, 126.17624733842673, 121.94173140693225, 137.80851461194607, 132.1409932917646, 121.99852647873877, 126.77328071144723, 155.55135316452575, 130.11918852725603, 126.35867072559775, 123.6841019016324, 126.10045134987686, 125.69227322502684, 146.65477710527537, 122.89495589187474, 158.17355305360263, 134.33817466588445, 125.23869606645336, 121.91830386069918, 122.53666279057543, 123.85114367181859, 130.36854206204254, 126.7146779549854, 124.06950757989534, 122.22448073611154, 122.67507048757311, 134.942709570499, 142.32722560773922, 137.97170214126007, 122.18033312306515, 159.3440252001432, 125.98204693429588, 127.6235490871518, 131.4102162927322, 125.16434101620086, 129.68000686201924, 126.46862347818313, 134.7874863142889, 125.70811476956531, 129.83910404303572, 126.66490140917116, 124.00364665005178, 121.90639907451953, 127.93680600089665, 122.97717436313866, 123.3641768109384, 131.59177717630632, 122.36757396500205, 122.56948351433644, 132.1214784397926, 135.93501975488545, 128.09480909817123, 123.39136564792865, 149.39903090942687, 170.99434912330113, 131.71102306721917, 121.9054223795754, 135.71156096134933, 148.81034330699296, 127.43022393462644, 126.6923128602701, 121.90475533168332, 124.92089835399516, 139.07645896461935, 124.54760139165087, 131.0136352083965, 127.3695538264292, 121.99175946948566, 128.32227642060153, 129.21811827067376, 139.776085851383, 123.17743679297207, 138.93812664195713, 188.3292774728073, 122.4644158210157, 131.0441870315901, 139.28798296120024, 124.24233045201552, 158.94520447035987, 122.76358981529611, 132.7845337962909, 139.70307467743945, 166.32497342661344, 123.91292670142055, 124.74649643898593, 122.30716179907158, 134.87909488615597, 121.9862877050064, 122.82875946211651, 121.94935589012746, 124.52409452892934, 139.06005015967108, 132.91189221648688, 122.29216248125167, 140.3711826305726, 127.09531085104081, 122.57226303353691, 139.5405154365686, 143.61399426799605, 124.21500906324268, 148.54013452327575, 128.81086402090818, 123.41936076400854, 121.97219410160523, 152.4296927727443, 125.0569200182208, 131.58029494733307, 129.0330016729311, 123.75473165011343, 124.48996532032808, 125.83714884819852, 132.71934742040227, 122.05621254230556, 124.72328629747778, 122.90451900494138, 123.9179376079332, 124.67333971986226, 121.90800165559986, 122.90529422936217, 122.41014732277776, 124.24738439053387, 122.61434259281052, 126.5089156840675, 129.58162944111575, 126.540309793867, 125.71381360618312, 134.87782856553787, 126.57505346094239, 130.50825930911552, 122.11029884933242, 125.51171575334988, 129.9843734304175, 133.14654588326343, 130.06828589384796, 125.8166363247343]\n"
          ]
        }
      ],
      "source": [
        "# 5. Guardamos la función de perdida en train y test en cada época.\n",
        " # Store the loss function for train and test at each epoch\n",
        "print(\"Final optimized parameters:\", optimized_theta)\n",
        "print(\"Train Loss:\", train_loss)\n",
        "print(\"Test Loss:\", test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77e7828d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss vs Epoch\n",
        "plt.figure(figsize=(10, 6))\n",
        "epochs_gd = range(len(gd_train_loss))\n",
        "epochs_sgd = range(len(sgd_train_loss))\n",
        "\n",
        "plt.plot(epochs_gd, gd_train_loss, label='GD Train Loss', color='blue')\n",
        "plt.plot(epochs_gd, gd_test_loss, label='GD Test Loss', color='skyblue')\n",
        "plt.plot(epochs_sgd, sgd_train_loss, label='SGD Train Loss', color='red')\n",
        "plt.plot(epochs_sgd, sgd_test_loss, label='SGD Test Loss', color='salmon')\n",
        "\n",
        "plt.title('Loss vs Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4c183e0",
      "metadata": {},
      "source": [
        "Analice:\n",
        "\n",
        "1. Cómo se comportan estos algoritmos? se puede ver la diferencia entre SGD y GD?.\n",
        "2. Cómo afecto el _learning rate_ a estos algoritmos? Realice una simulación del mismo cambiando el `lr`.\n",
        "3. Compare en una curva de Perdida vs Epoch los dos algoritmos. Nota algo interesante?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7686a399",
      "metadata": {},
      "source": [
        "### Ejercicio 7"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a07e7e1",
      "metadata": {},
      "source": [
        "En este ejercicio vamos a considerar la regresión logística como un problema de clasificación binaria.\n",
        "La implementación de la misma podemos considerar la siguiente:\n",
        "\n",
        "```python\n",
        "\n",
        "class LogisticRegressionSGD():\n",
        "    def __init__(self, lr=0.01, max_iter=1000, tol=1e-3, random_state=42):\n",
        "        self.lr = lr\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.random_state = random_state\n",
        "        self.weights = None\n",
        "        self.loss = None\n",
        "        self.loss_history = None\n",
        "        self.grad_history = None\n",
        "        self.theta_history = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model according to the given training data.\n",
        "\n",
        "        Params:\n",
        "        X : numpy.ndarray\n",
        "            The training input samples. A 2D array of shape (n_samples, n_features).\n",
        "\n",
        "        y : numpy.ndarray\n",
        "            The target values. An array of shape (n_samples,).\n",
        "\n",
        "        Returns:\n",
        "        self : LogisticRegressionSGD\n",
        "            The fitted model.\n",
        "        \"\"\"\n",
        "        np.random.seed(self.random_state)\n",
        "        self.weights = np.random.normal(size=X.shape[1])\n",
        "        self.loss_history = []\n",
        "        self.grad_history = []\n",
        "        self.theta_history = []\n",
        "\n",
        "        self.SGD(X, y)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _step(self, X, y):\n",
        "        \"\"\"\n",
        "        Perform a single gradient step.\n",
        "\n",
        "        Params:\n",
        "        X : numpy.ndarray\n",
        "            The training input samples. A 2D array of shape (n_samples, n_features).\n",
        "\n",
        "        y : numpy.ndarray\n",
        "            The target values. An array of shape (n_samples,).\n",
        "\n",
        "        Returns:\n",
        "        loss : float\n",
        "            The value of the loss function for the current value of the weights.\n",
        "\n",
        "        grad : numpy.ndarray\n",
        "            The gradient of the loss function for the current value of the weights.\n",
        "        \"\"\"\n",
        "        N = len(y)\n",
        "        y_hat = self.logit(X)\n",
        "        loss = -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
        "        grad = (-1 / N) * X.T.dot(y - y_hat)\n",
        "\n",
        "        return loss, grad\n",
        "\n",
        "    def SGD(self, X, y):\n",
        "        \"\"\"\n",
        "        Perform the stochastic gradient descent optimization algorithm.\n",
        "\n",
        "        Params:\n",
        "        X : numpy.ndarray\n",
        "            The training input samples. A 2D array of shape (n_samples, n_features).\n",
        "\n",
        "        y : numpy.ndarray\n",
        "            The target values. An array of shape (n_samples,).\n",
        "        \"\"\"\n",
        "        for epoch in range(self.max_iter):\n",
        "        prev_weights = self.weights.copy()\n",
        "        prev_loss = self._step(X, y)\n",
        "\n",
        "        self.weights -= self.lr * prev_loss[1]\n",
        "\n",
        "        self.loss_history.append(prev_loss[0])\n",
        "        self.grad_history.append(prev_loss[1])\n",
        "        self.theta_history.append(prev_weights)\n",
        "\n",
        "        # importante la convergencia para que no actualice innecesariamente los pesos\n",
        "        if np.linalg.norm(self.weights - prev_weights) < self.tol:\n",
        "            break\n",
        "\n",
        "    def logit(self, X):\n",
        "        \"\"\"\n",
        "        Calculate the logit of a set of observations.\n",
        "\n",
        "        Params:\n",
        "        X : numpy.ndarray\n",
        "            The training input samples. A 2D array of shape (n_samples, n_features).\n",
        "\n",
        "        Returns:\n",
        "        logit : numpy.ndarray\n",
        "            The logit of the observations. An array of shape (n_samples,).\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-X.dot(self.weights)))\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict the probability of each class for a set of observations.\n",
        "\n",
        "        Params:\n",
        "        X : numpy.ndarray\n",
        "            The training input samples. A 2D array of shape (n_samples, n_features).\n",
        "\n",
        "        Returns:\n",
        "        proba : numpy.ndarray\n",
        "            The predicted probability of each class. An array of shape (n_samples,).\n",
        "        \"\"\"\n",
        "        return self.logit(X)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the class of a set of observations.\n",
        "\n",
        "        Params:\n",
        "        X : numpy.ndarray\n",
        "            The training input samples. A 2D array of shape (n_samples, n_features).\n",
        "\n",
        "        Returns:\n",
        "        y_pred : numpy.ndarray\n",
        "            The predicted class. An array of shape (n_samples,).\n",
        "        \"\"\"\n",
        "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
        "    \n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        Calculate the accuracy of the model.\n",
        "\n",
        "        Params:\n",
        "        X : numpy.ndarray\n",
        "            The training input samples. A 2D array of shape (n_samples, n_features).\n",
        "\n",
        "        y : numpy.ndarray\n",
        "            The target values. An array of shape (n_samples,).\n",
        "\n",
        "        Returns:\n",
        "        score : float\n",
        "            The accuracy of the model.\n",
        "        \"\"\"\n",
        "        return np.mean(self.predict(X) == y)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con esta clase, vamos a tomar el dataset de breast cancer y vamos a realizar una clasificación binaria.\n",
        "La idea de este ejercicio es que puedan jugar con la manera de obtener los hiperparámetros óptimos para el modelo.\n",
        "\n",
        "Para ello van a tener que completar el método `SGD` de la clase `LogisticRegressionSGD` y luego realizar una búsqueda de grilla para encontrar los mejores hiperparámetros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddc71d51",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "X,y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'lr': [0.001, 0.01, 0.1], \n",
        "    'max_iter': [1000, 2000],  \n",
        "    'tol': [1e-3, 1e-4]  \n",
        "}\n",
        "\n",
        "model = LogisticRegressionSGD()\n",
        "\n",
        "# realizo grid search\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# mejores hiperparametros\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Best Accuracy:\", best_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid_search.best_estimator_\n",
        "test_accuracy = best_model.score(X_test, y_test)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Practica_clase_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "vscode": {
      "interpreter": {
        "hash": "b5c22da4a52024410f64f9c5a5e2b4ffeeb944a5ed00e8825a42174cdab30315"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
