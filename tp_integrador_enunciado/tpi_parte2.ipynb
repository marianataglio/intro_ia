{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "07289bb0",
      "metadata": {},
      "source": [
        "# Trabajo integrador - Parte 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42989be4",
      "metadata": {},
      "source": [
        "# Aprendizaje Supervisado\n",
        "\n",
        "**Nombre**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8387800e",
      "metadata": {},
      "source": [
        "## Problema de regresión"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "dc5e74de",
      "metadata": {},
      "source": [
        "Para la creación de los datasets y la manipulación de los mismos vamos a trabajar directamente con dos módulos includios en la carpeta utils.\n",
        "\n",
        "En esta podemos encontrar:\n",
        " - generate_data: Esta función wrappea el método de _make_regression_ de scikit learn para devolver un dataframe con un problema de regresión basado en sus parámetros.\n",
        " - generate_outliers: Esta función genera outliers livianos y pesados en función de los parámetros que le demos de entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a033541",
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.data_generation import generate_dataset\n",
        "from utils.data_manipulation import generate_outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abc333db",
      "metadata": {},
      "source": [
        "### Ejemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a902d24a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fee49e6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Vamos a crear un dataset primero.\n",
        "\n",
        "data = generate_dataset(\n",
        "    n_samples=1000,\n",
        "    n_features=5,\n",
        "    n_informative=2,\n",
        "    n_targets=1,\n",
        "    noise=0,\n",
        "    output='dataframe'\n",
        ")\n",
        "\n",
        "## esto nos genera un dataset que contiene 5 features, 2 de los cuales son informativos, y 1 target.\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c0df0dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "## vamos a visualizar estas variables\n",
        "## creamos una figura de matplotlib que contenga 5 subplots, uno por cada feature:\n",
        "\n",
        "fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
        "\n",
        "## Creamos un loop para iterar sobre cada feature y graficar la regresión lineal entre cada feature y el target:\n",
        "\n",
        "for i, feature in enumerate(data.columns[:-1]):\n",
        "    sns.regplot(x=feature,\n",
        "                y='target',\n",
        "                data=data,\n",
        "                ax=axes[i],\n",
        "                scatter_kws={'alpha': 0.4},\n",
        "                line_kws={'color': 'red'},\n",
        "                ci=95)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba9d4947",
      "metadata": {},
      "source": [
        "Ahora agregamos _outliers_ a un nuevo dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fcec748",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = generate_dataset(\n",
        "    n_samples=1000,\n",
        "    n_features=1,\n",
        "    n_informative=1,\n",
        "    n_targets=1,\n",
        "    noise=0,\n",
        "    output='dataframe'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f706cc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "do1 = generate_outliers(\n",
        "    df=data,\n",
        "    columns=['x0'],\n",
        "    percentage=0.01,\n",
        "    extreme_outliers=False,\n",
        "    only_tails=False,\n",
        ")\n",
        "do2 = generate_outliers(\n",
        "    df=data,\n",
        "    columns=['x0'],\n",
        "    percentage=0.01,\n",
        "    extreme_outliers=False,\n",
        "    only_tails=True,\n",
        "    two_tailed=True,\n",
        ")\n",
        "do3 = generate_outliers(\n",
        "    df=data,\n",
        "    columns=['x0'],\n",
        "    percentage=0.01,\n",
        "    extreme_outliers=False,\n",
        "    only_tails=True,\n",
        "    two_tailed=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37444567",
      "metadata": {},
      "outputs": [],
      "source": [
        "## vamos a visualizar estas los distintos datasets\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "sns.regplot(x='x0',\n",
        "            y='target',\n",
        "            data=data,\n",
        "            ax=axes[0],\n",
        "            scatter_kws={'alpha': 0.4},\n",
        "            line_kws={'color': 'red'},\n",
        "            ci=95)\n",
        "axes[0].set_title('Original')\n",
        "\n",
        "sns.regplot(x='x0',\n",
        "            y='target',\n",
        "            data=do1,\n",
        "            ax=axes[1],\n",
        "            scatter_kws={'alpha': 0.4},\n",
        "            line_kws={'color': 'red'},\n",
        "            ci=95)\n",
        "axes[1].set_title('Outliers')\n",
        "\n",
        "sns.regplot(x='x0',\n",
        "            y='target',\n",
        "            data=do2,\n",
        "            ax=axes[2],\n",
        "            scatter_kws={'alpha': 0.4},\n",
        "            line_kws={'color': 'red'},\n",
        "            ci=95)\n",
        "axes[2].set_title('Outliers (two-tailed)')\n",
        "\n",
        "sns.regplot(x='x0',\n",
        "            y='target',\n",
        "            data=do3,\n",
        "            ax=axes[3],\n",
        "            scatter_kws={'alpha': 0.4},\n",
        "            line_kws={'color': 'red'},\n",
        "            ci=95)\n",
        "axes[3].set_title('Outliers (one-tailed)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fcda0e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "## y si lo queremos con mucho mas outliers?\n",
        "\n",
        "doe = generate_outliers(\n",
        "    df=data,\n",
        "    columns=['x0'],\n",
        "    percentage=0.1,\n",
        "    extreme_outliers=True)\n",
        "\n",
        "## vamos a visualizar este caso\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
        "\n",
        "sns.regplot(x='x0',\n",
        "            y='target',\n",
        "            data=data,\n",
        "            ax=axes[0],\n",
        "            scatter_kws={'alpha': 0.4},\n",
        "            line_kws={'color': 'red'},\n",
        "            ci=95)\n",
        "axes[0].set_title('Original')\n",
        "\n",
        "sns.regplot(x='x0',\n",
        "            y='target',\n",
        "            data=doe,\n",
        "            ax=axes[1],\n",
        "            scatter_kws={'alpha': 0.4},\n",
        "            line_kws={'color': 'red'},\n",
        "            ci=95)\n",
        "axes[1].set_title('Outliers')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2734dfde",
      "metadata": {},
      "source": [
        "## Ejercicio 4"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "44621118",
      "metadata": {},
      "source": [
        "Utilizando la funcion `generate_data` generar un problema de regresión multivariada en el cual cuente con N variables informativas y M variables no informativas.\n",
        "\n",
        "Ejemplo:\n",
        "```python\n",
        "data = generate_dataset(n_samples=1000,\n",
        "                    n_features=10,\n",
        "                    n_informative=5,\n",
        "                    n_targets=1,\n",
        "                    noise=20.0,\n",
        "                    random_state=42,\n",
        "                    output='dataframe')\n",
        "\n",
        "```\n",
        "\n",
        "Dado un valor de _noise_ fijo, sin fijar _random_state_ (para poder asegurarnos\n",
        "que los datos que generamos son distintos) realizaremos 100 simulaciones de este dataset.\n",
        "\n",
        "En la simulación deberemos generar el dataset, hacer una división de train-test, ajustar\n",
        "un modelo de regresión lineal multivariada y validar el mismo.\n",
        "\n",
        "En cada iteración de esta simulación debemos guardar:\n",
        "\n",
        "- Los coeficientes de la regresión.\n",
        "- El RMSE de train y test.\n",
        "- El MAE de train y test. \n",
        "\n",
        "\n",
        "> Qué pasa con los coeficientes de las variables no informativas? La regresión se ve afectada por estas variables?\n",
        "> ***HINT:*** Utilice las distribuciones de los coeficientes para analizar y test de hipótesis para sacar conclusiones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "id": "eaa01488",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import mean_absolute_error, mean_absolute_error\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "id": "4df91762",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Parámetros de la simulación\n",
        "n_exp = 1000\n",
        "n_samples = 1000\n",
        "n_features = 10\n",
        "n_informative = 2\n",
        "n_targets = 1\n",
        "\n",
        "noise = np.linspace(0, 100, 100)\n",
        "bias = np.linspace(0, 100, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "id": "08e0cf86",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 4/1000 [00:45<3:08:58, 11.38s/it]"
          ]
        }
      ],
      "source": [
        "## Esqueleto de la simulación\n",
        "\n",
        "results = []\n",
        "for _ in tqdm(range(n_exp)):\n",
        "    for b in bias:\n",
        "        for n in noise:\n",
        "            data = generate_dataset(\n",
        "                n_samples=n_samples,\n",
        "                n_features=n_features,\n",
        "                n_informative=n_informative,\n",
        "                n_targets=n_targets,\n",
        "                noise=n,\n",
        "                bias=b,\n",
        "                output='dataframe'\n",
        "            )\n",
        "            # Train test split\n",
        "            X = data.drop('target', axis=1)\n",
        "            y = data['target']\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "            model = LinearRegression()\n",
        "            model.fit(X_train, y_train)\n",
        "            coeficientes = model.coef_\n",
        "\n",
        "            # Valido predicciones en train y test\n",
        "            y_train_pred = model.predict(X_train)\n",
        "            y_test_pred = model.predict(X_test)\n",
        "\n",
        "            # RMSE y MAE\n",
        "            rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "            rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "            mae_train = mean_absolute_error(y_train, y_train_pred)\n",
        "            mae_test = mean_absolute_error(y_test, y_test_pred)\n",
        "\n",
        "            # Results\n",
        "            results.append({\n",
        "                'Coeficientes': coeficientes,\n",
        "                'RMSE_train': rmse_train,\n",
        "                'RMSE_test': rmse_test,\n",
        "                'MAE_train': mae_train,\n",
        "                'MAE_test': mae_test \n",
        "            })\n",
        "      \n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3edbda19",
      "metadata": {},
      "source": [
        "> Qué pasa con los coeficientes de las variables no informativas? La regresión se ve afectada por estas variables?\n",
        "> ***HINT:*** Utilice las distribuciones de los coeficientes para analizar y test de hipótesis para sacar conclusiones."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5772f3b8",
      "metadata": {},
      "source": [
        "## Ejercicio 5"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ad2f9ee6",
      "metadata": {},
      "source": [
        "\n",
        "Utilizando la funcion `generate_outliers` generar puntos extremos dentro de los datos que generamos anteriormente. En este ejercicio dejar setteado `extreme_outliers` como `False` y observe como variando el porcentaje de los mismos la regresión comienza a afectarse.\n",
        "\n",
        "Pasos:\n",
        "\n",
        "1. Generamos un dataset de regresion lineal simple (1 feature y 1 target value) con `noise` fijo en 0.5.\n",
        "2. Generamos outliers fijando `extreme_outliers`.\n",
        "2. Probar los distintos regresores a ver como se comportan frente a estos datasets anómalos.\n",
        "3. Simular con multiples porcentajes de outliers (desde 1% hasta 10%). Qué pasa con los modelos?\n",
        "\n",
        "Los modelos a utilizar en este problema son:\n",
        "\n",
        "    - Regresion Lineal simple\n",
        "    - Regresion de Huber\n",
        "    - Regresión Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82a019af",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression, HuberRegressor, RidgeCV"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be7e8c85",
      "metadata": {},
      "source": [
        "## Problema de Clasificación"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53495bd5",
      "metadata": {},
      "source": [
        "### Ejercicio 6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9639ca5",
      "metadata": {},
      "source": [
        "En este ejercicio vamos a jugar un poco con descenso de gradiente. Para esto consideremos lo visto en clase que es el problema de regresión.\n",
        "\n",
        "Como paso inicial, vamos a sacarnos de encima la parte teórica. Recordemos que partimos del siguiente modelo\n",
        "\n",
        "$$\n",
        "y = \\beta_0 + \\beta_1 \\cdot x\n",
        "$$\n",
        "\n",
        "En este caso nuestra función objetivo a optimizar será:\n",
        "\n",
        "$$\n",
        "MSE = ||y-\\hat{y}||^2\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6deb44f7",
      "metadata": {},
      "source": [
        "Para calcular el gradiente de la función de error cuadrático medio (MSE) con respecto a los parámetros $\\beta_0$ y $\\beta_1$, es útil primero expresar la función de coste de forma más explicita. Dado que $\\hat{y} = \\beta_0 + \\beta_1 \\cdot x$, podemos reescribir la función MSE como sigue:\n",
        "\n",
        "$$\n",
        "MSE(\\beta_0, \\beta_1) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\beta_0 - \\beta_1 \\cdot x_i)^2\n",
        "$$\n",
        "\n",
        "Aquí, $N$ es el número de observaciones en el conjunto de datos y $y_i$ y $x_i$ son el valor observado y el valor de la característica correspondiente para la i-ésima observación.\n",
        "\n",
        "El gradiente de la función de coste está compuesto por las derivadas parciales de la función de coste con respecto a cada uno de los parámetros. Así, el gradiente es un vector de la forma:\n",
        "\n",
        "$$\n",
        "\\nabla MSE(\\beta_0, \\beta_1) = \\left[ \\frac{\\partial MSE}{\\partial \\beta_0}, \\frac{\\partial MSE}{\\partial \\beta_1} \\right]\n",
        "$$\n",
        "\n",
        "Las derivadas parciales se pueden calcular como sigue:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial MSE}{\\partial \\beta_0} = \\frac{-2}{N} \\sum_{i=1}^{N} (y_i - \\beta_0 - \\beta_1 \\cdot x_i)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial MSE}{\\partial \\beta_1} = \\frac{-2}{N} \\sum_{i=1}^{N} x_i \\cdot (y_i - \\beta_0 - \\beta_1 \\cdot x_i)\n",
        "$$\n",
        "\n",
        "Así que finalmente tenemos:\n",
        "\n",
        "$$\n",
        "\\nabla MSE(\\beta_0, \\beta_1) = \\left[ \\frac{-2}{N} \\sum_{i=1}^{N} (y_i - \\beta_0 - \\beta_1 \\cdot x_i), \\frac{-2}{N} \\sum_{i=1}^{N} x_i \\cdot (y_i - \\beta_0 - \\beta_1 \\cdot x_i) \\right]\n",
        "$$\n",
        "\n",
        "El cálculo del gradiente se usa en el descenso de gradiente para actualizar los parámetros $\\beta_0$ y $\\beta_1$ en cada iteración, en dirección opuesta al gradiente, para minimizar la función de coste.\n",
        "\n",
        "Estos cálculos se pueden implementar en código Python de la siguiente manera:\n",
        "\n",
        "```python\n",
        "def gradient(X, y, beta0, beta1):\n",
        "    N = len(y)\n",
        "    y_hat = beta0 + beta1 * X\n",
        "\n",
        "    d_beta0 = (-2/N) * np.sum(y - y_hat)\n",
        "    d_beta1 = (-2/N) * np.sum(X * (y - y_hat))\n",
        "\n",
        "    return d_beta0, d_beta1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ce7dee2",
      "metadata": {},
      "source": [
        "Ahora, si quisieramos realizar esto de manera matricial, podemos hacer lo siguiente:\n",
        "\n",
        "Primero, necesitamos cambiar la representación de nuestros datos. Podemos agregar un vector de unos a nuestra matriz de características para representar el término de intersección $\\beta_0$. De esta manera, $X$ toma esta forma:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & x_1 \\\\\n",
        "1 & x_2 \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "1 & x_N \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Y nuestro vector de parámetros $\\theta$ se verá así:\n",
        "\n",
        "$$\n",
        "\\theta = \\begin{bmatrix}\n",
        "\\beta_0 \\\\\n",
        "\\beta_1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Entonces, nuestra predicción $\\hat{y}$ se calcula como $X\\theta$:\n",
        "\n",
        "$$\n",
        "\\hat{y} = X\\theta = \\begin{bmatrix}\n",
        "1 & x_1 \\\\\n",
        "1 & x_2 \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "1 & x_N \\\\\n",
        "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
        "\\beta_0 \\\\\n",
        "\\beta_1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Nuestra función de coste MSE se ve de la siguiente manera en forma matricial:\n",
        "\n",
        "$$\n",
        "MSE(\\theta) = \\frac{1}{N} (y - X\\theta)^T (y - X\\theta)\n",
        "$$\n",
        "\n",
        "Las derivadas parciales de esta función de coste con respecto a los parámetros se pueden calcular de la siguiente manera:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial MSE}{\\partial \\theta} = \\frac{-2}{N} X^T (y - X\\theta)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c4a59fb",
      "metadata": {},
      "source": [
        "\n",
        "Esto se puede implementar en Python de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "b7ac568e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient(X: np.ndarray, y: np.ndarray, theta: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Esta función calcula el gradiente de la función de coste del error cuadrático medio (MSE)\n",
        "    para una regresión lineal simple. La función toma como entrada la matriz de características X,\n",
        "    el vector de observaciones y y el vector de parámetros theta, y devuelve el gradiente, que\n",
        "    es un vector de las mismas dimensiones que theta.\n",
        "\n",
        "    Params:\n",
        "    X : numpy.ndarray\n",
        "        La matriz de características extendida que incluye un vector de unos. De tamaño (N, d),\n",
        "        donde N es el número de observaciones y d es el número de características (incluyendo el\n",
        "        término de intersección).\n",
        "\n",
        "    y : numpy.ndarray\n",
        "        El vector de observaciones. De tamaño (N,), donde N es el número de observaciones.\n",
        "\n",
        "    theta : numpy.ndarray\n",
        "        El vector de parámetros. De tamaño (d,), donde d es el número de características\n",
        "        (incluyendo el término de intersección).\n",
        "\n",
        "    Returns:\n",
        "    grad : numpy.ndarray\n",
        "        El gradiente de la función de coste. Un vector de las mismas dimensiones que theta.\n",
        "\n",
        "    Examples:\n",
        "    >>> X = np.array([[1, 1], [1, 2], [1, 3]])\n",
        "    >>> y = np.array([2, 3, 4])\n",
        "    >>> theta = np.array([0, 0])\n",
        "    >>> gradient(X, y, theta)\n",
        "    array([-4., -8.])\n",
        "    \"\"\"\n",
        "    N = len(y)\n",
        "    y_hat = X.dot(theta)\n",
        "\n",
        "    grad = (-2 / N) * X.T.dot(y - y_hat)\n",
        "\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1ef2b1d",
      "metadata": {},
      "source": [
        "\n",
        "Aquí, `X` es la matriz de características extendida que incluye un vector de unos, `y` es el vector de observaciones, y `theta` es el vector de parámetros. La función devuelve el gradiente, que es un vector de las mismas dimensiones que `theta`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0885fd5a",
      "metadata": {},
      "source": [
        "#### Gradiente Descendente"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91e4a496",
      "metadata": {},
      "source": [
        "Ahora que sabemos calcular el gradiente, vamos a:\n",
        "\n",
        "1. Crear una función _GD_ que compute el gradiente descendente. Debe tener condición de frenado\n",
        "por nr de épocas pero también por tolerancia.\n",
        "2. Generamos un dataset (con _generate_dataset_ de los ejercicios anteriores, utilizando un bias conocido y solo 1 feature)\n",
        "3. Inicializamos un vector $(\\beta_0, \\beta_1)$ al azar.\n",
        "4. Tratamos de calcular los mejores parámetros con el algoritmo.\n",
        "5. Guardamos la función de perdida en train y test en cada época."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "id": "e2a249e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. C1. Crear una función _GD_ que compute el gradiente descendente. Debe tener condición de frenado por nr de épocas pero también por tolerancia.\n",
        "def GD(X_train, y_train, X_test, y_test, initial_theta, learning_rate, max_epochs=100, tolerance=1e-5):\n",
        "    theta = initial_theta.copy()\n",
        "    train_loss = []\n",
        "    test_loss = []\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        # Computo gradientes\n",
        "        grad = gradient(X_train, y_train, theta)\n",
        "\n",
        "        # actualizo parametros con gd\n",
        "        theta -= learning_rate * grad\n",
        "\n",
        "        # Calculo losses\n",
        "        train_pred = X_train.dot(theta)\n",
        "        train_mse = np.mean((y_train - train_pred) ** 2)\n",
        "        train_loss.append(train_mse)\n",
        "\n",
        "        test_pred = X_test.dot(theta)\n",
        "        test_mse = np.mean((y_test - test_pred) ** 2)\n",
        "        test_loss.append(test_mse)\n",
        "\n",
        "        # chequear que se cumpla la condicion de frenado por tolerancia\n",
        "        if epoch > 0 and abs(train_loss[-1] - train_loss[-2]) < tolerance:\n",
        "            break\n",
        "\n",
        "    return train_loss, test_loss, theta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "id": "e867284e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Generamos un dataset (con _generate_dataset_ de los ejercicios anteriores, utilizando un bias conocido y solo 1 feature)\n",
        "data = generate_dataset(n_samples=1000,\n",
        "                    n_features=1,\n",
        "                    n_informative=1,\n",
        "                    n_targets=1,\n",
        "                    bias = 5,\n",
        "                    random_state=42,\n",
        "                    output='dataframe')\n",
        "\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "# print(\"Shapes - X_train:\", X_train.shape, \"y_train:\", y_train.shape, \"X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n",
        "\n",
        "\n",
        "X_train = X_train.values.reshape(-1, 1)\n",
        "y_train = y_train.values.ravel()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "id": "d675279f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Inicializamos un vector $(\\beta_0, \\beta_1)$ al azar.\n",
        "initial_theta = np.random.rand(X_train.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "id": "2642dc53",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Tratamos de calcular los mejores parámetros con el algoritmo.\n",
        "learning_rate = 0.1\n",
        "max_epochs = 100\n",
        "tolerance = 1e-5\n",
        "gd_train_loss, gd_test_loss, optimized_theta = GD(X_train, y_train, X_test, y_test, initial_theta, learning_rate, max_epochs, tolerance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "id": "6bf5fea5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final optimized parameters: [16.79197762]\n",
            "Train Loss: [287.0340841814546, 229.08413827089223, 191.71669279783376, 167.62131256508889, 152.0840623942377, 142.0652894784423, 135.6049564314411, 131.43918648845974, 128.75300311500317, 127.02089087294532, 125.90398543427229, 125.18377935948857, 124.71937407306461, 124.41991496322127, 124.22681690622305, 124.1023028794633, 124.02201339804157, 123.97024091107986, 123.93685683188617, 123.91533001698866, 123.90144903626899, 123.89249826363228, 123.88672660149888, 123.88300490225188, 123.88060506578105, 123.87905759654603, 123.87805975312523, 123.87741632095236, 123.8770014212271, 123.87673388443105, 123.87656137060932, 123.87645012976645, 123.87637839913644, 123.87633214559187, 123.87630232025248, 123.87628308819407, 123.87627068692464, 123.87626269030356]\n",
            "Test Loss: [255.42771629774353, 206.79551874824188, 175.66884594155323, 155.78433851953687, 143.11226104535518, 135.06139254307334, 129.9666718819365, 126.75910096404554, 124.75311651854055, 123.50966386089766, 122.74804929531967, 122.28921742308381, 122.01926892037733, 121.8660113420564, 121.78389906986227, 121.74437084937546, 121.72965824108952, 121.72882450793362, 121.7352355654521, 121.7449494011773, 121.75569376299956, 121.76621998011952, 121.77589675590714, 121.78445663883024, 121.79183928844878, 121.79809582791093, 121.80333151908756, 121.80767229194123, 121.81124596741137, 121.81417240235157, 121.81655894381844, 121.81849895058036, 121.82007200627477, 121.82134499358364, 121.82237353908488, 121.8232035489485, 121.82387268422386, 121.82441170162592]\n"
          ]
        }
      ],
      "source": [
        "# 5. Guardamos la función de perdida en train y test en cada época.\n",
        " # Store the loss function for train and test at each epoch\n",
        "print(\"Final optimized parameters:\", optimized_theta)\n",
        "print(\"Train Loss:\", gd_train_loss)\n",
        "print(\"Test Loss:\", gd_test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e02fba8",
      "metadata": {},
      "source": [
        "#### Gradiente Descendente Estocástico"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f767cd27",
      "metadata": {},
      "source": [
        "Ahora que sabemos calcular el gradiente, vamos a:\n",
        "\n",
        "1. Crear una función _SGD_ que compute el gradiente descendente estocástico.\n",
        "2. Generamos un dataset (con _generate_dataset_ de los ejercicios anteriores, utilizando un bias conocido y solo 1 feature)\n",
        "3. Inicializamos un vector $(\\beta_0, \\beta_1)$ al azar.\n",
        "4. Tratamos de calcular los mejores parámetros con el algoritmo.\n",
        "5. Guardamos la función de perdida en train y test en cada época."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "id": "0e80946d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Crear una función _SGD_ que compute el gradiente descendente estocástico.\n",
        "def SGD(X_train, y_train, X_test, y_test, initial_theta, learning_rate, max_epochs=100, tolerance=1e-5, batch_size=1):\n",
        "    theta = initial_theta.copy()\n",
        "    train_loss = []\n",
        "    test_loss = []\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "\n",
        "        indices = np.random.permutation(len(X_train))\n",
        "        X_train_shuffled = X_train[indices]\n",
        "        y_train_shuffled = y_train[indices]\n",
        "\n",
        "        for i in range(0, len(X_train), batch_size):\n",
        "            if i + batch_size <= len(X_train):\n",
        "                X_batch = X_train_shuffled[i:i+batch_size]\n",
        "                y_batch = y_train_shuffled[i:i+batch_size]\n",
        "            else:\n",
        "                X_batch = X_train_shuffled[i:]\n",
        "                y_batch = y_train_shuffled[i:]\n",
        "\n",
        "            grad = gradient(X_batch, y_batch, theta)\n",
        "\n",
        "            # actualizo params usando sgd\n",
        "            theta -= learning_rate * grad\n",
        "\n",
        "        # Calculo losses para cada epoch\n",
        "        train_pred = X_train.dot(theta)\n",
        "        train_mse = np.mean((y_train - train_pred) ** 2)\n",
        "        train_loss.append(train_mse)\n",
        "\n",
        "        test_pred = X_test.dot(theta)\n",
        "        test_mse = np.mean((y_test - test_pred) ** 2)\n",
        "        test_loss.append(test_mse)\n",
        "\n",
        "        # condicion de frenado\n",
        "        if epoch > 0 and abs(train_loss[-1] - train_loss[-2]) < tolerance:\n",
        "            break\n",
        "\n",
        "    return train_loss, test_loss, theta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "id": "34060e38",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes - X_train: (800, 1) y_train: (800,) X_test: (200, 1) y_test: (200,)\n"
          ]
        }
      ],
      "source": [
        "# 2. Generamos un dataset (con _generate_dataset_ de los ejercicios anteriores, utilizando un bias conocido y solo 1 feature)\n",
        "data = generate_dataset(n_samples=1000,\n",
        "                    n_features=1,\n",
        "                    n_informative=1,\n",
        "                    n_targets=1,\n",
        "                    bias = 5,\n",
        "                    random_state=42,\n",
        "                    output='dataframe')\n",
        "\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "print(\"Shapes - X_train:\", X_train.shape, \"y_train:\", y_train.shape, \"X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n",
        "\n",
        "X_train = X_train.values.reshape(-1, 1)\n",
        "y_train = y_train.values.ravel()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "id": "289cee18",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Inicializamos un vector $(\\beta_0, \\beta_1)$ al azar.\n",
        "initial_theta = np.random.rand(X_train.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "id": "fc586dc9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Tratamos de calcular los mejores parámetros con el algoritmo.\n",
        "learning_rate = 0.1\n",
        "max_epochs = 100\n",
        "tolerance = 1e-5\n",
        "batch_size = 2\n",
        "sgd_train_loss, sgd_test_loss, optimized_theta = SGD(X_train, y_train, X_test, y_test, initial_theta, learning_rate, max_epochs, tolerance, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "id": "d28b6fb1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final optimized parameters: [20.49188426]\n",
            "Train Loss: [143.7548316301943, 140.07171151161216, 126.24035150593744, 136.7923447549933, 123.83159188622204, 138.3207230613791, 129.79377267111744, 127.138831858404, 131.98381498711544, 129.53400750845014, 125.436088593155, 142.06113209497016, 123.26501651874192, 173.85948761469155, 124.72711998818359, 131.58641537675746, 128.19417361033666, 123.2388676896932, 124.6900997905205, 125.20810206719756, 125.81807337955283, 134.09763436844517, 128.8142555408567, 173.69061582084515, 123.28003374276076, 147.74784730531047, 123.18800869866482, 124.10986468297394, 124.15734317258388, 124.59848422742024, 123.11162456976228, 132.79198060003588, 123.81468769446987, 139.44018108705868, 126.23037539017916, 161.72101776594727, 128.59262234694745, 131.81181755900275, 125.85802033590136, 150.75875738022057, 164.05237270963426, 125.79220141794609, 135.52453436371474, 126.07622192097843, 129.24321521544354, 123.13602604359883, 124.65947410626235, 141.9381624383449, 135.71516618046903, 123.25596227011849, 125.14107480997406, 127.3365179816437, 128.89294779746746, 126.80011562002126, 132.48927454759712, 136.8530264387103, 134.7997351129468, 156.21917730604656, 130.02206042122566, 126.21081590212954, 123.1699032997305, 123.26216453179877, 126.2782306750516, 138.5453205050314, 127.68129615504425, 123.26323830678498, 124.6040479336854, 123.19526574208496, 127.78631142842224, 123.86444252438366, 133.6322881114291, 137.63684199742914, 125.03939409307019, 123.17987000152917, 123.33152230795277, 126.38277902493304, 172.0669113017648, 124.09818724156757, 128.16512941851326, 126.41868709677809, 152.80484569884229, 123.52639250035209, 123.45888297720809, 124.46615424214367, 123.75645344383918, 123.53457004708042, 136.95846455066834, 123.09046443789657, 123.2481374676398, 190.14985558881418, 123.09012816805327, 130.8256282881582, 126.27521016152598, 141.8456164669421, 125.85451927010222, 126.51719331809969, 123.24753364369514, 133.1968952849708, 139.28958949153804, 137.00154278378955]\n",
            "Test Loss: [143.14786595909362, 143.24833159793658, 127.28127929667602, 136.73108462684516, 125.32545397248869, 138.1330086860949, 130.39323806309386, 128.05480694444287, 134.89339186723237, 132.32067946061386, 127.90475392292475, 145.28414917820098, 125.3191259029799, 171.34815458873555, 127.10762787784468, 134.47800760348164, 130.8980834211618, 125.27991504409756, 125.98936753208909, 127.65044605340279, 128.32748953496014, 137.09312991963313, 129.52358690663164, 177.2373983477141, 125.34118483755556, 146.85526748284843, 125.19966283787795, 125.53308150869444, 125.56941260804425, 126.96064518551897, 125.055808076475, 133.08754649758862, 126.03774349908925, 139.16253780867552, 127.2727663199319, 165.20718390366818, 129.3277840957715, 134.71369077643124, 126.95646566140084, 154.13294134471028, 162.1124456609904, 128.29897498646457, 138.57037552730944, 127.14146880854611, 132.01306067880702, 124.92241387586125, 127.03044504908694, 145.158468035366, 135.74592963860752, 125.30567213118564, 126.35718269191975, 128.22655695427443, 129.59320077263726, 129.3998484728188, 132.81378325367734, 139.94127035122054, 137.8206779443273, 159.65758024269158, 132.83575598931913, 127.25608102990657, 125.16922470610486, 124.96064259573565, 127.31362008876985, 141.68230909784745, 128.5272180529525, 124.96114054801001, 125.92027861259695, 124.93380500234082, 128.6190578675757, 126.0983696719153, 133.84915399737662, 137.50515359180042, 126.27350895613489, 124.92923230680377, 125.41481838513009, 127.40301974511247, 169.65763545617105, 125.52418075768827, 128.95127869698933, 127.43377016868932, 151.57018879268617, 125.1136034056422, 125.58792278116172, 126.80844867901483, 125.27135524131013, 125.68678997474859, 140.04990904804015, 124.98912749794479, 124.9542994588752, 193.71228272773052, 124.98707202176202, 133.68085601122115, 127.3110402210078, 141.38131704101966, 128.36763132817745, 129.09268138271938, 125.29303373638308, 133.45424716881274, 139.02392159026311, 136.9227021503555]\n"
          ]
        }
      ],
      "source": [
        "# 5. Guardamos la función de perdida en train y test en cada época.\n",
        " # Store the loss function for train and test at each epoch\n",
        "print(\"Final optimized parameters:\", optimized_theta)\n",
        "print(\"Train Loss:\", sgd_train_loss)\n",
        "print(\"Test Loss:\", sgd_test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "id": "77e7828d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Loss vs Epoch\n",
        "plt.figure(figsize=(10, 6))\n",
        "epochs_gd = range(len(gd_train_loss))\n",
        "epochs_sgd = range(len(sgd_train_loss))\n",
        "\n",
        "#plt.plot(epochs_gd, gd_train_loss, label='GD Train Loss', color='blue')\n",
        "plt.plot(epochs_gd, gd_test_loss, label='GD Test Loss', color='skyblue')\n",
        "#plt.plot(epochs_sgd, sgd_train_loss, label='SGD Train Loss', color='red')\n",
        "plt.plot(epochs_sgd, sgd_test_loss, label='SGD Test Loss', color='salmon')\n",
        "\n",
        "plt.title('Loss vs Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4c183e0",
      "metadata": {},
      "source": [
        "Analice:\n",
        "\n",
        "1. Cómo se comportan estos algoritmos? se puede ver la diferencia entre SGD y GD?.\n",
        "2. Cómo afecto el _learning rate_ a estos algoritmos? Realice una simulación del mismo cambiando el `lr`.\n",
        "3. Compare en una curva de Perdida vs Epoch los dos algoritmos. Nota algo interesante?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7686a399",
      "metadata": {},
      "source": [
        "### Ejercicio 7"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a07e7e1",
      "metadata": {},
      "source": [
        "En este ejercicio vamos a considerar la regresión logística como un problema de clasificación binaria.\n",
        "La implementación de la misma podemos considerar la siguiente:\n",
        "\n",
        "```python\n",
        "\n",
        "class LogisticRegressionSGD():\n",
        "    def __init__(self, lr=0.01, max_iter=1000, tol=1e-3, random_state=42):\n",
        "        self.lr = lr\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.random_state = random_state\n",
        "        self.weights = None\n",
        "        self.loss = None\n",
        "        self.loss_history = None\n",
        "        self.grad_history = None\n",
        "        self.theta_history = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model according to the given training data.\n",
        "\n",
        "        Params:\n",
        "        X : numpy.ndarray\n",
        "            The training input samples. A 2D array of shape (n_samples, n_features).\n",
        "\n",
        "        y : numpy.ndarray\n",
        "            The target values. An array of shape (n_samples,).\n",
        "\n",
        "        Returns:\n",
        "        self : LogisticRegressionSGD\n",
        "            The fitted model.\n",
        "        \"\"\"\n",
        "        np.random.seed(self.random_state)\n",
        "        self.weights = np.random.normal(size=X.shape[1])\n",
        "        self.loss_history = []\n",
        "        self.grad_history = []\n",
        "        self.theta_history = []\n",
        "\n",
        "        self.SGD(X, y)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _step(self, X, y):\n",
        "        \"\"\"\n",
        "        Perform a single gradient step.\n",
        "\n",
        "        Params:\n",
        "        X : numpy.ndarray\n",
        "            The training input samples. A 2D array of shape (n_samples, n_features).\n",
        "\n",
        "        y : numpy.ndarray\n",
        "            The target values. An array of shape (n_samples,).\n",
        "\n",
        "        Returns:\n",
        "        loss : float\n",
        "            The value of the loss function for the current value of the weights.\n",
        "\n",
        "        grad : numpy.ndarray\n",
        "            The gradient of the loss function for the current value of the weights.\n",
        "        \"\"\"\n",
        "        N = len(y)\n",
        "        y_hat = self.logit(X)\n",
        "        loss = -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
        "        grad = (-1 / N) * X.T.dot(y - y_hat)\n",
        "\n",
        "        return loss, grad\n",
        "\n",
        "    def SGD(self, X, y):\n",
        "        \"\"\"\n",
        "        Perform the stochastic gradient descent optimization algorithm.\n",
        "\n",
        "        Params:\n",
        "        X : numpy.ndarray\n",
        "            The training input samples. A 2D array of shape (n_samples, n_features).\n",
        "\n",
        "        y : numpy.ndarray\n",
        "            The target values. An array of shape (n_samples,).\n",
        "        \"\"\"\n",
        "        for epoch in range(self.max_iter):\n",
        "        prev_weights = self.weights.copy()\n",
        "        prev_loss = self._step(X, y)\n",
        "\n",
        "        self.weights -= self.lr * prev_loss[1]\n",
        "\n",
        "        self.loss_history.append(prev_loss[0])\n",
        "        self.grad_history.append(prev_loss[1])\n",
        "        self.theta_history.append(prev_weights)\n",
        "\n",
        "        # importante la convergencia para que no actualice innecesariamente los pesos\n",
        "        if np.linalg.norm(self.weights - prev_weights) < self.tol:\n",
        "            break\n",
        "\n",
        "    def logit(self, X):\n",
        "        \"\"\"\n",
        "        Calculate the logit of a set of observations.\n",
        "\n",
        "        Params:\n",
        "        X : numpy.ndarray\n",
        "            The training input samples. A 2D array of shape (n_samples, n_features).\n",
        "\n",
        "        Returns:\n",
        "        logit : numpy.ndarray\n",
        "            The logit of the observations. An array of shape (n_samples,).\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-X.dot(self.weights)))\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict the probability of each class for a set of observations.\n",
        "\n",
        "        Params:\n",
        "        X : numpy.ndarray\n",
        "            The training input samples. A 2D array of shape (n_samples, n_features).\n",
        "\n",
        "        Returns:\n",
        "        proba : numpy.ndarray\n",
        "            The predicted probability of each class. An array of shape (n_samples,).\n",
        "        \"\"\"\n",
        "        return self.logit(X)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the class of a set of observations.\n",
        "\n",
        "        Params:\n",
        "        X : numpy.ndarray\n",
        "            The training input samples. A 2D array of shape (n_samples, n_features).\n",
        "\n",
        "        Returns:\n",
        "        y_pred : numpy.ndarray\n",
        "            The predicted class. An array of shape (n_samples,).\n",
        "        \"\"\"\n",
        "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
        "    \n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        Calculate the accuracy of the model.\n",
        "\n",
        "        Params:\n",
        "        X : numpy.ndarray\n",
        "            The training input samples. A 2D array of shape (n_samples, n_features).\n",
        "\n",
        "        y : numpy.ndarray\n",
        "            The target values. An array of shape (n_samples,).\n",
        "\n",
        "        Returns:\n",
        "        score : float\n",
        "            The accuracy of the model.\n",
        "        \"\"\"\n",
        "        return np.mean(self.predict(X) == y)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con esta clase, vamos a tomar el dataset de breast cancer y vamos a realizar una clasificación binaria.\n",
        "La idea de este ejercicio es que puedan jugar con la manera de obtener los hiperparámetros óptimos para el modelo.\n",
        "\n",
        "Para ello van a tener que completar el método `SGD` de la clase `LogisticRegressionSGD` y luego realizar una búsqueda de grilla para encontrar los mejores hiperparámetros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddc71d51",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "X,y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'lr': [0.001, 0.01, 0.1], \n",
        "    'max_iter': [1000, 2000],  \n",
        "    'tol': [1e-3, 1e-4]  \n",
        "}\n",
        "\n",
        "model = LogisticRegressionSGD()\n",
        "\n",
        "# realizo grid search\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# mejores hiperparametros\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Best Accuracy:\", best_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid_search.best_estimator_\n",
        "test_accuracy = best_model.score(X_test, y_test)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Practica_clase_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "vscode": {
      "interpreter": {
        "hash": "b5c22da4a52024410f64f9c5a5e2b4ffeeb944a5ed00e8825a42174cdab30315"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
